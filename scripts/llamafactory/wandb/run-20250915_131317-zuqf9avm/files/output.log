  2%|█▏                                                                  | 50/2967 [13:31<12:35:56, 15.55s/it][INFO|trainer.py:4226] 2025-09-15 13:26:50,505 >>
{'loss': 1.5327, 'grad_norm': 8.421522566289767, 'learning_rate': 6.734006734006734e-08, 'epoch': 0.01}
{'loss': 1.5146, 'grad_norm': 8.02601567815275, 'learning_rate': 1.3468013468013468e-07, 'epoch': 0.02}
{'loss': 1.5082, 'grad_norm': 6.933283379915658, 'learning_rate': 2.02020202020202e-07, 'epoch': 0.03}
{'loss': 1.4202, 'grad_norm': 5.5739857438274685, 'learning_rate': 2.6936026936026936e-07, 'epoch': 0.04}
{'loss': 1.3931, 'grad_norm': 4.26153076524063, 'learning_rate': 3.3670033670033673e-07, 'epoch': 0.05}
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-09-15 13:26:50,505 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-09-15 13:26:50,505 >>   Batch size = 2
  3%|██████▏                                                                                                                                                                                 | 100/2967 [30:48<13:02:56, 16.39s/it][INFO|trainer.py:4226] 2025-09-15 13:44:07,513 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 1.202689290046692, 'eval_runtime': 239.583, 'eval_samples_per_second': 6.954, 'eval_steps_per_second': 0.438, 'epoch': 0.05}
{'loss': 1.2299, 'grad_norm': 3.0293415394614533, 'learning_rate': 4.04040404040404e-07, 'epoch': 0.06}
{'loss': 1.211, 'grad_norm': 2.3360576257330057, 'learning_rate': 4.7138047138047136e-07, 'epoch': 0.07}
{'loss': 1.1229, 'grad_norm': 2.120596862230834, 'learning_rate': 5.387205387205387e-07, 'epoch': 0.08}
{'loss': 1.085, 'grad_norm': 2.1388664354511735, 'learning_rate': 6.060606060606061e-07, 'epoch': 0.09}
{'loss': 1.0264, 'grad_norm': 2.1213612748895883, 'learning_rate': 6.734006734006735e-07, 'epoch': 0.1}
[INFO|trainer.py:4228] 2025-09-15 13:44:07,513 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-09-15 13:44:07,513 >>   Batch size = 2
  5%|█████████▎                                                                                                                                                                              | 150/2967 [48:08<12:57:04, 16.55s/it][INFO|trainer.py:4226] 2025-09-15 14:01:27,193 >>
***** Running Evaluation *****                                                                                                                                                                                                     
{'eval_loss': 0.9769519567489624, 'eval_runtime': 235.5269, 'eval_samples_per_second': 7.074, 'eval_steps_per_second': 0.446, 'epoch': 0.1}
{'loss': 1.0243, 'grad_norm': 1.9651799270994514, 'learning_rate': 7.407407407407406e-07, 'epoch': 0.11}
{'loss': 0.9863, 'grad_norm': 2.1829294437569184, 'learning_rate': 8.08080808080808e-07, 'epoch': 0.12}
{'loss': 1.0008, 'grad_norm': 1.9628259235525882, 'learning_rate': 8.754208754208754e-07, 'epoch': 0.13}
{'loss': 1.0041, 'grad_norm': 1.9508694690604325, 'learning_rate': 9.427609427609427e-07, 'epoch': 0.14}
{'loss': 0.9823, 'grad_norm': 1.8745022183665099, 'learning_rate': 1.01010101010101e-06, 'epoch': 0.15}
[INFO|trainer.py:4228] 2025-09-15 14:01:27,193 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-09-15 14:01:27,193 >>   Batch size = 2
  7%|████████████▎                                                                                                                                                                         | 200/2967 [1:05:39<11:41:51, 15.22s/it][INFO|trainer.py:4226] 2025-09-15 14:18:58,632 >>
***** Running Evaluation *****                                                                                                                                                                                                     
{'eval_loss': 0.9189748764038086, 'eval_runtime': 240.21, 'eval_samples_per_second': 6.936, 'eval_steps_per_second': 0.437, 'epoch': 0.15}
{'loss': 0.955, 'grad_norm': 1.8506634221541545, 'learning_rate': 1.0774410774410775e-06, 'epoch': 0.16}
{'loss': 0.9624, 'grad_norm': 1.86168679061321, 'learning_rate': 1.1447811447811446e-06, 'epoch': 0.17}
{'loss': 0.9559, 'grad_norm': 1.7721281106787596, 'learning_rate': 1.2121212121212122e-06, 'epoch': 0.18}
{'loss': 0.9632, 'grad_norm': 1.9604918575035584, 'learning_rate': 1.2794612794612794e-06, 'epoch': 0.19}
{'loss': 0.9256, 'grad_norm': 2.1088763667419226, 'learning_rate': 1.346801346801347e-06, 'epoch': 0.2}
[INFO|trainer.py:4228] 2025-09-15 14:18:58,632 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-09-15 14:18:58,632 >>   Batch size = 2
  7%|████████████▎                                                                                                                                                                         | 200/2967 [1:09:40<11:41:51, 15.22s/it][INFO|trainer.py:3910] 2025-09-15 14:23:07,194 >> Saving model checkpoint to /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-200
[INFO|configuration_utils.py:420] 2025-09-15 14:23:07,219 >> Configuration saved in /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-200/config.json
{'eval_loss': 0.8935410380363464, 'eval_runtime': 240.6176, 'eval_samples_per_second': 6.924, 'eval_steps_per_second': 0.436, 'epoch': 0.2}
[INFO|configuration_utils.py:909] 2025-09-15 14:23:07,227 >> Configuration saved in /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-200/generation_config.json
[INFO|modeling_utils.py:2996] 2025-09-15 14:23:21,711 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-200/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2491] 2025-09-15 14:23:21,719 >> tokenizer config file saved in /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-09-15 14:23:21,727 >> Special tokens file saved in /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-200/special_tokens_map.json
[2025-09-15 14:23:21,896] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is about to be saved!
[2025-09-15 14:23:21,922] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-09-15 14:23:21,923] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-09-15 14:23:21,970] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-09-15 14:23:22,046] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-09-15 14:23:58,206] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-09-15 14:23:58,216] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-09-15 14:24:00,311] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
  8%|███████████████▎                                                                                                                                                                      | 250/2967 [1:23:58<12:18:54, 16.32s/it][INFO|trainer.py:4226] 2025-09-15 14:37:17,781 >>
{'loss': 0.9317, 'grad_norm': 2.0837955788676705, 'learning_rate': 1.414141414141414e-06, 'epoch': 0.21}
{'loss': 0.9679, 'grad_norm': 1.7222271976536874, 'learning_rate': 1.4814814814814812e-06, 'epoch': 0.22}
{'loss': 0.9402, 'grad_norm': 1.8235535385292225, 'learning_rate': 1.5488215488215488e-06, 'epoch': 0.23}
{'loss': 0.935, 'grad_norm': 1.890717643283116, 'learning_rate': 1.616161616161616e-06, 'epoch': 0.24}
{'loss': 0.9332, 'grad_norm': 1.8753900724455925, 'learning_rate': 1.6835016835016836e-06, 'epoch': 0.25}
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-09-15 14:37:17,781 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-09-15 14:37:17,781 >>   Batch size = 2
 10%|██████████████████▍                                                                                                                                                                   | 300/2967 [1:41:15<11:59:26, 16.19s/it][INFO|trainer.py:4226] 2025-09-15 14:54:34,590 >>
***** Running Evaluation *****                                                                                                                                                                                                     
{'eval_loss': 0.8745341897010803, 'eval_runtime': 239.8743, 'eval_samples_per_second': 6.945, 'eval_steps_per_second': 0.438, 'epoch': 0.25}
{'loss': 0.9102, 'grad_norm': 1.8062044487163944, 'learning_rate': 1.7508417508417507e-06, 'epoch': 0.26}
{'loss': 0.94, 'grad_norm': 1.614217269037489, 'learning_rate': 1.818181818181818e-06, 'epoch': 0.27}
{'loss': 0.9148, 'grad_norm': 2.0613937925264536, 'learning_rate': 1.8855218855218854e-06, 'epoch': 0.28}
{'loss': 0.9055, 'grad_norm': 1.819356266838706, 'learning_rate': 1.9528619528619526e-06, 'epoch': 0.29}
{'loss': 0.9181, 'grad_norm': 1.7409671915451574, 'learning_rate': 1.999993769982229e-06, 'epoch': 0.3}
[INFO|trainer.py:4228] 2025-09-15 14:54:34,590 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-09-15 14:54:34,590 >>   Batch size = 2
 12%|█████████████████████▍                                                                                                                                                                | 350/2967 [1:58:36<11:35:14, 15.94s/it][INFO|trainer.py:4226] 2025-09-15 15:11:55,432 >>
***** Running Evaluation *****                                                                                                                                                                                                     
{'eval_loss': 0.862238347530365, 'eval_runtime': 240.0952, 'eval_samples_per_second': 6.939, 'eval_steps_per_second': 0.437, 'epoch': 0.3}
{'loss': 0.91, 'grad_norm': 1.714265143338518, 'learning_rate': 1.999883016270207e-06, 'epoch': 0.31}
{'loss': 0.9104, 'grad_norm': 1.9753589868969008, 'learning_rate': 1.99963383536794e-06, 'epoch': 0.32}
{'loss': 0.8989, 'grad_norm': 1.7089903994610285, 'learning_rate': 1.999246261772876e-06, 'epoch': 0.33}
{'loss': 0.8846, 'grad_norm': 2.0687334703288083, 'learning_rate': 1.9987203491420157e-06, 'epoch': 0.34}
{'loss': 0.94, 'grad_norm': 1.7745275293759453, 'learning_rate': 1.9980561702844845e-06, 'epoch': 0.35}
[INFO|trainer.py:4228] 2025-09-15 15:11:55,433 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-09-15 15:11:55,433 >>   Batch size = 2
 13%|████████████████████████▌                                                                                                                                                             | 400/2967 [2:15:43<10:54:48, 15.31s/it][INFO|trainer.py:4226] 2025-09-15 15:29:02,263 >>
***** Running Evaluation *****                                                                                                                                                                                                     
{'eval_loss': 0.8525503873825073, 'eval_runtime': 240.4881, 'eval_samples_per_second': 6.928, 'eval_steps_per_second': 0.437, 'epoch': 0.35}
{'loss': 0.8955, 'grad_norm': 1.6243576612645922, 'learning_rate': 1.997253817151452e-06, 'epoch': 0.36}
{'loss': 0.9042, 'grad_norm': 1.7529071486949634, 'learning_rate': 1.996313400823403e-06, 'epoch': 0.37}
{'loss': 0.8886, 'grad_norm': 1.8935025094479057, 'learning_rate': 1.9952350514947576e-06, 'epoch': 0.38}
{'loss': 0.8774, 'grad_norm': 1.8271952780556988, 'learning_rate': 1.9940189184558483e-06, 'epoch': 0.39}
{'loss': 0.8856, 'grad_norm': 2.0185809394397407, 'learning_rate': 1.992665170072251e-06, 'epoch': 0.4}
[INFO|trainer.py:4228] 2025-09-15 15:29:02,263 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-09-15 15:29:02,263 >>   Batch size = 2
 13%|████████████████████████▌                                                                                                                                                             | 400/2967 [2:19:43<10:54:48, 15.31s/it][INFO|trainer.py:3910] 2025-09-15 15:33:10,604 >> Saving model checkpoint to /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-400
[INFO|configuration_utils.py:420] 2025-09-15 15:33:10,621 >> Configuration saved in /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-400/config.json
{'eval_loss': 0.8464511036872864, 'eval_runtime': 240.5211, 'eval_samples_per_second': 6.927, 'eval_steps_per_second': 0.437, 'epoch': 0.4}
[INFO|configuration_utils.py:909] 2025-09-15 15:33:10,629 >> Configuration saved in /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-400/generation_config.json
[INFO|modeling_utils.py:2996] 2025-09-15 15:33:25,729 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-400/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2491] 2025-09-15 15:33:25,737 >> tokenizer config file saved in /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-09-15 15:33:25,745 >> Special tokens file saved in /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-400/special_tokens_map.json
[2025-09-15 15:33:25,985] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step400 is about to be saved!
[2025-09-15 15:33:26,010] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-400/global_step400/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-09-15 15:33:26,011] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-400/global_step400/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-09-15 15:33:26,079] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-400/global_step400/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-09-15 15:33:26,138] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-09-15 15:34:04,116] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-09-15 15:34:04,126] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-09-15 15:34:04,788] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
 15%|███████████████████████████▌                                                                                                                                                          | 450/2967 [2:34:13<11:08:04, 15.93s/it][INFO|trainer.py:4226] 2025-09-15 15:47:32,774 >>
{'loss': 0.8862, 'grad_norm': 1.5809677236713753, 'learning_rate': 1.9911739937614747e-06, 'epoch': 0.41}
{'loss': 0.9264, 'grad_norm': 1.8237800015902124, 'learning_rate': 1.9895455959670174e-06, 'epoch': 0.42}
{'loss': 0.92, 'grad_norm': 2.198579419209106, 'learning_rate': 1.987780202129783e-06, 'epoch': 0.43}
{'loss': 0.9033, 'grad_norm': 1.6948022739271424, 'learning_rate': 1.985878056656872e-06, 'epoch': 0.44}
{'loss': 0.9022, 'grad_norm': 1.9079715927083223, 'learning_rate': 1.983839422887744e-06, 'epoch': 0.46}
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-09-15 15:47:32,774 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-09-15 15:47:32,775 >>   Batch size = 2
 17%|██████████████████████████████▋                                                                                                                                                       | 500/2967 [2:51:30<10:15:18, 14.96s/it][INFO|trainer.py:4226] 2025-09-15 16:04:49,792 >>
***** Running Evaluation *****                                                                                                                                                                                                     
{'eval_loss': 0.8398000001907349, 'eval_runtime': 235.7997, 'eval_samples_per_second': 7.065, 'eval_steps_per_second': 0.445, 'epoch': 0.46}
{'loss': 0.8959, 'grad_norm': 1.6956576029448918, 'learning_rate': 1.9816645830577604e-06, 'epoch': 0.47}
{'loss': 0.8729, 'grad_norm': 1.751038973684814, 'learning_rate': 1.9793538382591113e-06, 'epoch': 0.48}
{'loss': 0.8803, 'grad_norm': 2.013367966527009, 'learning_rate': 1.97690750839913e-06, 'epoch': 0.49}
{'loss': 0.8678, 'grad_norm': 1.5989768554114547, 'learning_rate': 1.974325932156006e-06, 'epoch': 0.5}
{'loss': 0.9019, 'grad_norm': 1.8397181264636433, 'learning_rate': 1.9716094669318944e-06, 'epoch': 0.51}
[INFO|trainer.py:4228] 2025-09-15 16:04:49,792 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-09-15 16:04:49,792 >>   Batch size = 2
 19%|█████████████████████████████████▋                                                                                                                                                    | 550/2967 [3:09:07<10:58:11, 16.34s/it][INFO|trainer.py:4226] 2025-09-15 16:22:26,248 >>
***** Running Evaluation *****                                                                                                                                                                                                     
{'eval_loss': 0.8367273211479187, 'eval_runtime': 235.5158, 'eval_samples_per_second': 7.074, 'eval_steps_per_second': 0.446, 'epoch': 0.51}
{'loss': 0.8769, 'grad_norm': 1.7545348591832874, 'learning_rate': 1.968758488803439e-06, 'epoch': 0.52}
{'loss': 0.899, 'grad_norm': 1.7969931716956793, 'learning_rate': 1.9657733924697047e-06, 'epoch': 0.53}
{'loss': 0.8998, 'grad_norm': 1.6262156131704542, 'learning_rate': 1.9626545911975353e-06, 'epoch': 0.54}
{'loss': 0.8868, 'grad_norm': 1.5968797500523737, 'learning_rate': 1.9594025167643384e-06, 'epoch': 0.55}
{'loss': 0.8908, 'grad_norm': 1.792420918285619, 'learning_rate': 1.95601761939831e-06, 'epoch': 0.56}
[INFO|trainer.py:4228] 2025-09-15 16:22:26,248 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-09-15 16:22:26,249 >>   Batch size = 2
 20%|████████████████████████████████████▊                                                                                                                                                 | 600/2967 [3:26:32<10:37:22, 16.16s/it][INFO|trainer.py:4226] 2025-09-15 16:39:51,583 >>
***** Running Evaluation *****                                                                                                                                                                                                     
{'eval_loss': 0.8313787579536438, 'eval_runtime': 239.7728, 'eval_samples_per_second': 6.948, 'eval_steps_per_second': 0.438, 'epoch': 0.56}
{'loss': 0.8954, 'grad_norm': 1.81763482467034, 'learning_rate': 1.9525003677161025e-06, 'epoch': 0.57}
{'loss': 0.8681, 'grad_norm': 1.8200999187975273, 'learning_rate': 1.948851248657947e-06, 'epoch': 0.58}
{'loss': 0.8649, 'grad_norm': 1.6639088221130511, 'learning_rate': 1.9450707674202416e-06, 'epoch': 0.59}
{'loss': 0.8918, 'grad_norm': 1.5784328159414323, 'learning_rate': 1.9411594473856097e-06, 'epoch': 0.6}
{'loss': 0.8795, 'grad_norm': 2.1183338077747984, 'learning_rate': 1.9371178300504393e-06, 'epoch': 0.61}
[INFO|trainer.py:4228] 2025-09-15 16:39:51,583 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-09-15 16:39:51,583 >>   Batch size = 2
 20%|████████████████████████████████████▊                                                                                                                                                 | 600/2967 [3:30:34<10:37:22, 16.16s/it][INFO|trainer.py:3910] 2025-09-15 16:44:03,497 >> Saving model checkpoint to /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-600
[INFO|configuration_utils.py:420] 2025-09-15 16:44:03,514 >> Configuration saved in /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-600/config.json
{'eval_loss': 0.8269888758659363, 'eval_runtime': 242.1033, 'eval_samples_per_second': 6.881, 'eval_steps_per_second': 0.434, 'epoch': 0.61}
[INFO|configuration_utils.py:909] 2025-09-15 16:44:03,522 >> Configuration saved in /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-600/generation_config.json
[INFO|modeling_utils.py:2996] 2025-09-15 16:44:17,804 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-600/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2491] 2025-09-15 16:44:17,813 >> tokenizer config file saved in /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-09-15 16:44:17,820 >> Special tokens file saved in /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-600/special_tokens_map.json
[2025-09-15 16:44:18,473] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step600 is about to be saved!
[2025-09-15 16:44:18,498] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-600/global_step600/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-09-15 16:44:18,498] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-600/global_step600/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-09-15 16:44:18,550] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-600/global_step600/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-09-15 16:44:18,621] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-09-15 16:44:54,846] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-09-15 16:44:54,855] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /apdcephfs_cq11_1567347/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_cog-sft_balance_single_alf_lr2e6_bs32_epoch3_full_0915/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-09-15 16:44:55,581] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step600 is ready now!
 21%|█████████████████████████████████████▉                                                                                                                                                | 618/2967 [3:36:24<10:36:10, 16.25s/it]
{'loss': 0.8725, 'grad_norm': 1.7367841553772598, 'learning_rate': 1.9329464749499186e-06, 'epoch': 0.62}
