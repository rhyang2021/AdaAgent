  6%|████                                                                    | 50/885 [01:34<25:56,  1.86s/it][INFO|trainer.py:4226] 2025-06-26 12:29:40,991 >>
{'loss': 1.4595, 'grad_norm': 1.498526282965256, 'learning_rate': 2.2471910112359549e-07, 'epoch': 0.06}
{'loss': 1.4316, 'grad_norm': 1.4788585752742642, 'learning_rate': 4.4943820224719097e-07, 'epoch': 0.11}
{'loss': 1.4714, 'grad_norm': 1.486253314389745, 'learning_rate': 6.741573033707865e-07, 'epoch': 0.17}
{'loss': 1.4354, 'grad_norm': 1.5493867605013518, 'learning_rate': 8.988764044943819e-07, 'epoch': 0.23}
{'loss': 1.4313, 'grad_norm': 1.7177626479677175, 'learning_rate': 1.1235955056179775e-06, 'epoch': 0.28}
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-06-26 12:29:40,992 >>   Num examples = 90
[INFO|trainer.py:4231] 2025-06-26 12:29:40,992 >>   Batch size = 2
 11%|████████                                                               | 100/885 [03:08<23:09,  1.77s/it][INFO|trainer.py:4226] 2025-06-26 12:31:15,185 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 1.4340709447860718, 'eval_runtime': 6.5331, 'eval_samples_per_second': 13.776, 'eval_steps_per_second': 0.918, 'epoch': 0.28}
{'loss': 1.4203, 'grad_norm': 1.6374849001631848, 'learning_rate': 1.348314606741573e-06, 'epoch': 0.34}
{'loss': 1.45, 'grad_norm': 1.8269084366165087, 'learning_rate': 1.5730337078651686e-06, 'epoch': 0.4}
{'loss': 1.4416, 'grad_norm': 1.6976981656059456, 'learning_rate': 1.7977528089887639e-06, 'epoch': 0.45}
{'loss': 1.4138, 'grad_norm': 1.719184040079541, 'learning_rate': 1.99999221169321e-06, 'epoch': 0.51}
{'loss': 1.388, 'grad_norm': 1.63080212982101, 'learning_rate': 1.999057761661208e-06, 'epoch': 0.56}
[INFO|trainer.py:4228] 2025-06-26 12:31:15,185 >>   Num examples = 90
[INFO|trainer.py:4231] 2025-06-26 12:31:15,186 >>   Batch size = 2
 17%|████████████                                                           | 150/885 [04:44<21:50,  1.78s/it][INFO|trainer.py:4226] 2025-06-26 12:32:51,455 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 1.3499661684036255, 'eval_runtime': 6.5638, 'eval_samples_per_second': 13.712, 'eval_steps_per_second': 0.914, 'epoch': 0.56}
{'loss': 1.3212, 'grad_norm': 1.3803007061739885, 'learning_rate': 1.9965673179314084e-06, 'epoch': 0.62}
{'loss': 1.2905, 'grad_norm': 1.341838988707259, 'learning_rate': 1.9925247592732856e-06, 'epoch': 0.68}
{'loss': 1.246, 'grad_norm': 1.0917143854149127, 'learning_rate': 1.986936381815087e-06, 'epoch': 0.73}
{'loss': 1.2025, 'grad_norm': 0.8585968183601445, 'learning_rate': 1.9798108892378605e-06, 'epoch': 0.79}
{'loss': 1.1729, 'grad_norm': 0.8482907346391483, 'learning_rate': 1.971159379219809e-06, 'epoch': 0.85}
[INFO|trainer.py:4228] 2025-06-26 12:32:51,455 >>   Num examples = 90
[INFO|trainer.py:4231] 2025-06-26 12:32:51,455 >>   Batch size = 2
 23%|████████████████                                                       | 200/885 [06:18<19:23,  1.70s/it][INFO|trainer.py:4226] 2025-06-26 12:34:25,229 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 1.1613045930862427, 'eval_runtime': 6.5429, 'eval_samples_per_second': 13.755, 'eval_steps_per_second': 0.917, 'epoch': 0.85}
{'loss': 1.1632, 'grad_norm': 0.7739922235483498, 'learning_rate': 1.9609953261520836e-06, 'epoch': 0.9}
{'loss': 1.127, 'grad_norm': 0.8657458702662507, 'learning_rate': 1.9493345601529468e-06, 'epoch': 0.96}
{'loss': 1.1313, 'grad_norm': 0.7821265922603812, 'learning_rate': 1.936195242412975e-06, 'epoch': 1.02}
{'loss': 1.1052, 'grad_norm': 0.6474504369650348, 'learning_rate': 1.9215978369097084e-06, 'epoch': 1.07}
{'loss': 1.0631, 'grad_norm': 0.5529514524888588, 'learning_rate': 1.905565078535802e-06, 'epoch': 1.13}
[INFO|trainer.py:4228] 2025-06-26 12:34:25,229 >>   Num examples = 90
[INFO|trainer.py:4231] 2025-06-26 12:34:25,229 >>   Batch size = 2
 23%|████████████████                                                       | 200/885 [06:25<19:23,  1.70s/it][INFO|trainer.py:3910] 2025-06-26 12:34:37,440 >> Saving model checkpoint to /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-200
[INFO|configuration_utils.py:694] 2025-06-26 12:34:37,474 >> loading configuration file /apdcephfs_cq11/share_1567347/share_info/llm_models/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json
{'eval_loss': 1.0514484643936157, 'eval_runtime': 6.5466, 'eval_samples_per_second': 13.747, 'eval_steps_per_second': 0.916, 'epoch': 1.13}
[INFO|configuration_utils.py:768] 2025-06-26 12:34:37,475 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.3",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2491] 2025-06-26 12:34:37,593 >> tokenizer config file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-06-26 12:34:37,601 >> Special tokens file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-200/special_tokens_map.json
[2025-06-26 12:34:37,784] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is about to be saved!
[2025-06-26 12:34:38,486] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-06-26 12:34:38,486] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-06-26 12:34:38,508] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-06-26 12:34:38,516] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-06-26 12:34:38,681] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-06-26 12:34:38,692] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-06-26 12:34:38,706] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
 28%|████████████████████                                                   | 250/885 [07:57<18:26,  1.74s/it][INFO|trainer.py:4226] 2025-06-26 12:36:04,765 >>
{'loss': 1.0303, 'grad_norm': 0.4882108865224598, 'learning_rate': 1.888121937690312e-06, 'epoch': 1.19}
{'loss': 1.0182, 'grad_norm': 0.5111312867262795, 'learning_rate': 1.8692955813882661e-06, 'epoch': 1.24}
{'loss': 1.0221, 'grad_norm': 0.5087785348390751, 'learning_rate': 1.8491153309490942e-06, 'epoch': 1.3}
{'loss': 1.0175, 'grad_norm': 0.5239832743627827, 'learning_rate': 1.82761261632981e-06, 'epoch': 1.36}
{'loss': 0.9735, 'grad_norm': 0.40159368482009655, 'learning_rate': 1.8048209271740734e-06, 'epoch': 1.41}
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-06-26 12:36:04,765 >>   Num examples = 90
[INFO|trainer.py:4231] 2025-06-26 12:36:04,765 >>   Batch size = 2
 34%|████████████████████████                                               | 300/885 [09:31<16:22,  1.68s/it][INFO|trainer.py:4226] 2025-06-26 12:37:38,693 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.9840790629386902, 'eval_runtime': 6.5383, 'eval_samples_per_second': 13.765, 'eval_steps_per_second': 0.918, 'epoch': 1.41}
{'loss': 0.984, 'grad_norm': 0.5148207753782794, 'learning_rate': 1.780775760653368e-06, 'epoch': 1.47}
{'loss': 0.9528, 'grad_norm': 0.3709976075294115, 'learning_rate': 1.7555145661815368e-06, 'epoch': 1.53}
{'loss': 0.9689, 'grad_norm': 0.4806802920964193, 'learning_rate': 1.7290766870887702e-06, 'epoch': 1.58}
{'loss': 0.9117, 'grad_norm': 0.34537214977659464, 'learning_rate': 1.7015032993458985e-06, 'epoch': 1.64}
{'loss': 0.9455, 'grad_norm': 0.4891710607326825, 'learning_rate': 1.6728373474344135e-06, 'epoch': 1.69}
[INFO|trainer.py:4228] 2025-06-26 12:37:38,693 >>   Num examples = 90
[INFO|trainer.py:4231] 2025-06-26 12:37:38,694 >>   Batch size = 2
 40%|████████████████████████████                                           | 350/885 [11:08<17:46,  1.99s/it][INFO|trainer.py:4226] 2025-06-26 12:39:15,015 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.9446129202842712, 'eval_runtime': 6.5678, 'eval_samples_per_second': 13.703, 'eval_steps_per_second': 0.914, 'epoch': 1.69}
{'loss': 0.9114, 'grad_norm': 0.35686882056929564, 'learning_rate': 1.6431234774621045e-06, 'epoch': 1.75}
{'loss': 0.9174, 'grad_norm': 0.4111660138067499, 'learning_rate': 1.6124079676284802e-06, 'epoch': 1.81}
{'loss': 0.9255, 'grad_norm': 0.4884174148144997, 'learning_rate': 1.5807386561482661e-06, 'epoch': 1.86}
{'loss': 0.9296, 'grad_norm': 0.35213409292804476, 'learning_rate': 1.5481648667452425e-06, 'epoch': 1.92}
{'loss': 0.9154, 'grad_norm': 0.42222281282649704, 'learning_rate': 1.5147373318324586e-06, 'epoch': 1.98}
[INFO|trainer.py:4228] 2025-06-26 12:39:15,016 >>   Num examples = 90
[INFO|trainer.py:4231] 2025-06-26 12:39:15,016 >>   Batch size = 2
 45%|████████████████████████████████                                       | 400/885 [12:43<14:12,  1.76s/it][INFO|trainer.py:4226] 2025-06-26 12:40:50,686 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.9184476733207703, 'eval_runtime': 6.5354, 'eval_samples_per_second': 13.771, 'eval_steps_per_second': 0.918, 'epoch': 1.98}
{'loss': 0.9193, 'grad_norm': 0.3823126838499738, 'learning_rate': 1.4805081134984672e-06, 'epoch': 2.03}
{'loss': 0.8975, 'grad_norm': 0.3530303737597801, 'learning_rate': 1.4455305224226427e-06, 'epoch': 2.09}
{'loss': 0.8831, 'grad_norm': 0.3410814963586342, 'learning_rate': 1.4098590348458656e-06, 'epoch': 2.15}
{'loss': 0.8812, 'grad_norm': 0.3045604983928163, 'learning_rate': 1.3735492077258924e-06, 'epoch': 2.2}
{'loss': 0.9034, 'grad_norm': 0.34679102077164353, 'learning_rate': 1.3366575922095483e-06, 'epoch': 2.26}
[INFO|trainer.py:4228] 2025-06-26 12:40:50,686 >>   Num examples = 90
[INFO|trainer.py:4231] 2025-06-26 12:40:50,686 >>   Batch size = 2
 45%|████████████████████████████████                                       | 400/885 [12:50<14:12,  1.76s/it][INFO|trainer.py:3910] 2025-06-26 12:41:02,256 >> Saving model checkpoint to /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-400
[INFO|configuration_utils.py:694] 2025-06-26 12:41:02,282 >> loading configuration file /apdcephfs_cq11/share_1567347/share_info/llm_models/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json
{'eval_loss': 0.9000778794288635, 'eval_runtime': 6.5319, 'eval_samples_per_second': 13.778, 'eval_steps_per_second': 0.919, 'epoch': 2.26}
[INFO|configuration_utils.py:768] 2025-06-26 12:41:02,283 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.3",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2491] 2025-06-26 12:41:02,396 >> tokenizer config file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-06-26 12:41:02,404 >> Special tokens file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-400/special_tokens_map.json
[2025-06-26 12:41:02,586] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step400 is about to be saved!
[2025-06-26 12:41:02,611] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-400/global_step400/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-06-26 12:41:02,612] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-400/global_step400/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-06-26 12:41:03,289] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-400/global_step400/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-06-26 12:41:03,298] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-06-26 12:41:03,452] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-06-26 12:41:03,463] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-06-26 12:41:03,477] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
 51%|████████████████████████████████████                                   | 450/885 [14:24<12:19,  1.70s/it][INFO|trainer.py:4226] 2025-06-26 12:42:31,076 >>
{'loss': 0.8726, 'grad_norm': 0.34648556436767997, 'learning_rate': 1.2992416455565112e-06, 'epoch': 2.32}
{'loss': 0.8687, 'grad_norm': 0.3379092184054521, 'learning_rate': 1.2613596416518593e-06, 'epoch': 2.37}
{'loss': 0.8839, 'grad_norm': 0.3595900046333936, 'learning_rate': 1.2230705802467555e-06, 'epoch': 2.43}
{'loss': 0.8854, 'grad_norm': 0.38867838407893035, 'learning_rate': 1.1844340950686249e-06, 'epoch': 2.49}
{'loss': 0.8598, 'grad_norm': 0.44648429056573, 'learning_rate': 1.1455103609439386e-06, 'epoch': 2.54}
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-06-26 12:42:31,077 >>   Num examples = 90
[INFO|trainer.py:4231] 2025-06-26 12:42:31,077 >>   Batch size = 2
 56%|████████████████████████████████████████                               | 500/885 [15:58<11:40,  1.82s/it][INFO|trainer.py:4226] 2025-06-26 12:44:05,467 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.8850905895233154, 'eval_runtime': 6.5574, 'eval_samples_per_second': 13.725, 'eval_steps_per_second': 0.915, 'epoch': 2.54}
{'loss': 0.8843, 'grad_norm': 0.3443190503455932, 'learning_rate': 1.106360000078255e-06, 'epoch': 2.6}
{'loss': 0.8553, 'grad_norm': 0.36032137603370906, 'learning_rate': 1.067043987639489e-06, 'epoch': 2.66}
{'loss': 0.8686, 'grad_norm': 0.4049582552652259, 'learning_rate': 1.0276235567914521e-06, 'epoch': 2.71}
{'loss': 0.8788, 'grad_norm': 0.31242748566577777, 'learning_rate': 9.88160103325577e-07, 'epoch': 2.77}
{'loss': 0.8626, 'grad_norm': 0.36502875930380224, 'learning_rate': 9.487150900393544e-07, 'epoch': 2.82}
[INFO|trainer.py:4228] 2025-06-26 12:44:05,467 >>   Num examples = 90
[INFO|trainer.py:4231] 2025-06-26 12:44:05,467 >>   Batch size = 2
 62%|████████████████████████████████████████████                           | 550/885 [17:31<09:27,  1.69s/it][INFO|trainer.py:4226] 2025-06-26 12:45:38,297 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.8734572529792786, 'eval_runtime': 6.5347, 'eval_samples_per_second': 13.773, 'eval_steps_per_second': 0.918, 'epoch': 2.82}
{'loss': 0.8597, 'grad_norm': 0.288433860828532, 'learning_rate': 9.093499510104101e-07, 'epoch': 2.88}
{'loss': 0.872, 'grad_norm': 0.37981118621134113, 'learning_rate': 8.701259959153138e-07, 'epoch': 2.94}
{'loss': 0.8797, 'grad_norm': 0.345519725256194, 'learning_rate': 8.311043145421368e-07, 'epoch': 2.99}
{'loss': 0.8604, 'grad_norm': 0.3405276114804911, 'learning_rate': 7.923456816454767e-07, 'epoch': 3.05}
{'loss': 0.836, 'grad_norm': 0.33284143990991005, 'learning_rate': 7.539104622921367e-07, 'epoch': 3.11}
[INFO|trainer.py:4228] 2025-06-26 12:45:38,297 >>   Num examples = 90
[INFO|trainer.py:4231] 2025-06-26 12:45:38,297 >>   Batch size = 2
 68%|████████████████████████████████████████████████▏                      | 600/885 [19:02<07:34,  1.60s/it][INFO|trainer.py:4226] 2025-06-26 12:47:09,684 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.864633321762085, 'eval_runtime': 6.5581, 'eval_samples_per_second': 13.723, 'eval_steps_per_second': 0.915, 'epoch': 3.11}
{'loss': 0.8644, 'grad_norm': 0.4010438018912019, 'learning_rate': 7.158585178448748e-07, 'epoch': 3.16}
{'loss': 0.8487, 'grad_norm': 0.35318593922526603, 'learning_rate': 6.782491127306552e-07, 'epoch': 3.22}
{'loss': 0.8682, 'grad_norm': 0.3158180654827591, 'learning_rate': 6.41140822138602e-07, 'epoch': 3.28}
{'loss': 0.861, 'grad_norm': 0.32898915912897286, 'learning_rate': 6.045914407914165e-07, 'epoch': 3.33}
{'loss': 0.839, 'grad_norm': 0.3405585844883431, 'learning_rate': 5.686578929323377e-07, 'epoch': 3.39}
[INFO|trainer.py:4228] 2025-06-26 12:47:09,684 >>   Num examples = 90
[INFO|trainer.py:4231] 2025-06-26 12:47:09,684 >>   Batch size = 2
 68%|████████████████████████████████████████████████▏                      | 600/885 [19:09<07:34,  1.60s/it][INFO|trainer.py:3910] 2025-06-26 12:47:21,525 >> Saving model checkpoint to /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-600
[INFO|configuration_utils.py:694] 2025-06-26 12:47:21,559 >> loading configuration file /apdcephfs_cq11/share_1567347/share_info/llm_models/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json
{'eval_loss': 0.8580336570739746, 'eval_runtime': 6.5476, 'eval_samples_per_second': 13.746, 'eval_steps_per_second': 0.916, 'epoch': 3.39}
[INFO|configuration_utils.py:768] 2025-06-26 12:47:21,559 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.3",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2491] 2025-06-26 12:47:21,670 >> tokenizer config file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-06-26 12:47:21,678 >> Special tokens file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-600/special_tokens_map.json
[2025-06-26 12:47:21,856] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step600 is about to be saved!
[2025-06-26 12:47:21,881] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-600/global_step600/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-06-26 12:47:21,882] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-600/global_step600/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-06-26 12:47:22,641] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-600/global_step600/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-06-26 12:47:22,649] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-06-26 12:47:22,830] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-06-26 12:47:22,840] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-06-26 12:47:22,854] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step600 is ready now!
 73%|████████████████████████████████████████████████████▏                  | 650/885 [20:46<07:28,  1.91s/it][INFO|trainer.py:4226] 2025-06-26 12:48:53,346 >>
{'loss': 0.8481, 'grad_norm': 0.30911137498195346, 'learning_rate': 5.333961436678421e-07, 'epoch': 3.45}
{'loss': 0.8385, 'grad_norm': 0.3486567656473975, 'learning_rate': 4.988611118041644e-07, 'epoch': 3.5}
{'loss': 0.8314, 'grad_norm': 0.3182541863439144, 'learning_rate': 4.6510658431338367e-07, 'epoch': 3.56}
{'loss': 0.8217, 'grad_norm': 0.3612432610064894, 'learning_rate': 4.3218513256230624e-07, 'epoch': 3.62}
{'loss': 0.8581, 'grad_norm': 0.42671941454583456, 'learning_rate': 4.001480304345972e-07, 'epoch': 3.67}
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-06-26 12:48:53,347 >>   Num examples = 90
[INFO|trainer.py:4231] 2025-06-26 12:48:53,347 >>   Batch size = 2
 79%|████████████████████████████████████████████████████████▏              | 700/885 [22:21<05:25,  1.76s/it][INFO|trainer.py:4226] 2025-06-26 12:50:28,104 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.8533756136894226, 'eval_runtime': 6.5403, 'eval_samples_per_second': 13.761, 'eval_steps_per_second': 0.917, 'epoch': 3.67}
{'loss': 0.8284, 'grad_norm': 0.3362658896344532, 'learning_rate': 3.690451744736999e-07, 'epoch': 3.73}
{'loss': 0.8734, 'grad_norm': 0.37451276667812583, 'learning_rate': 3.3892500617090247e-07, 'epoch': 3.79}
{'loss': 0.8431, 'grad_norm': 0.49184546119554723, 'learning_rate': 3.09834436519598e-07, 'epoch': 3.84}
{'loss': 0.8372, 'grad_norm': 0.3058291746231899, 'learning_rate': 2.818187729532292e-07, 'epoch': 3.9}
{'loss': 0.8224, 'grad_norm': 0.3649624529944713, 'learning_rate': 2.549216487807223e-07, 'epoch': 3.95}
[INFO|trainer.py:4228] 2025-06-26 12:50:28,104 >>   Num examples = 90
[INFO|trainer.py:4231] 2025-06-26 12:50:28,104 >>   Batch size = 2
 85%|████████████████████████████████████████████████████████████▏          | 750/885 [23:55<03:54,  1.74s/it][INFO|trainer.py:4226] 2025-06-26 12:52:01,858 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.8507247567176819, 'eval_runtime': 6.5337, 'eval_samples_per_second': 13.775, 'eval_steps_per_second': 0.918, 'epoch': 3.95}
{'loss': 0.8368, 'grad_norm': 0.42935600265459056, 'learning_rate': 2.2918495522929814e-07, 'epoch': 4.01}
{'loss': 0.8479, 'grad_norm': 0.4271443272527677, 'learning_rate': 2.0464877620051457e-07, 'epoch': 4.07}
{'loss': 0.831, 'grad_norm': 0.35930681760469174, 'learning_rate': 1.8135132584114166e-07, 'epoch': 4.12}
{'loss': 0.8327, 'grad_norm': 0.4209126819007149, 'learning_rate': 1.5932888902611453e-07, 'epoch': 4.18}
{'loss': 0.8329, 'grad_norm': 0.3600776910797412, 'learning_rate': 1.3861576484624504e-07, 'epoch': 4.24}
[INFO|trainer.py:4228] 2025-06-26 12:52:01,858 >>   Num examples = 90
[INFO|trainer.py:4231] 2025-06-26 12:52:01,858 >>   Batch size = 2
 90%|████████████████████████████████████████████████████████████████▏      | 800/885 [25:26<02:31,  1.78s/it][INFO|trainer.py:4226] 2025-06-26 12:53:33,179 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.848744809627533, 'eval_runtime': 6.5288, 'eval_samples_per_second': 13.785, 'eval_steps_per_second': 0.919, 'epoch': 4.24}
{'loss': 0.8526, 'grad_norm': 0.323129150128437, 'learning_rate': 1.192442131887218e-07, 'epoch': 4.29}
{'loss': 0.8571, 'grad_norm': 0.4149844845944053, 'learning_rate': 1.0124440449358551e-07, 'epoch': 4.35}
{'loss': 0.8237, 'grad_norm': 0.2877973868058033, 'learning_rate': 8.464437276444059e-08, 'epoch': 4.41}
{'loss': 0.8376, 'grad_norm': 0.4849159422629908, 'learning_rate': 6.946997190658155e-08, 'epoch': 4.46}
{'loss': 0.8347, 'grad_norm': 0.32656591953122205, 'learning_rate': 5.5744835460538653e-08, 'epoch': 4.52}
[INFO|trainer.py:4228] 2025-06-26 12:53:33,180 >>   Num examples = 90
[INFO|trainer.py:4231] 2025-06-26 12:53:33,180 >>   Batch size = 2
 90%|████████████████████████████████████████████████████████████████▏      | 800/885 [25:32<02:31,  1.78s/it][INFO|trainer.py:3910] 2025-06-26 12:53:45,194 >> Saving model checkpoint to /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-800
[INFO|configuration_utils.py:694] 2025-06-26 12:53:45,227 >> loading configuration file /apdcephfs_cq11/share_1567347/share_info/llm_models/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json
{'eval_loss': 0.8476912379264832, 'eval_runtime': 6.5387, 'eval_samples_per_second': 13.764, 'eval_steps_per_second': 0.918, 'epoch': 4.52}
[INFO|configuration_utils.py:768] 2025-06-26 12:53:45,227 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.3",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2491] 2025-06-26 12:53:45,343 >> tokenizer config file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-06-26 12:53:45,350 >> Special tokens file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-800/special_tokens_map.json
[2025-06-26 12:53:45,528] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step800 is about to be saved!
[2025-06-26 12:53:45,552] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-800/global_step800/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-06-26 12:53:45,552] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-800/global_step800/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-06-26 12:53:46,338] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-800/global_step800/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-06-26 12:53:46,346] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-800/global_step800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-06-26 12:53:46,496] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-800/global_step800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-06-26 12:53:46,505] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-800/global_step800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-06-26 12:53:46,519] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step800 is ready now!
[INFO|trainer.py:4002] 2025-06-26 12:53:46,613 >> Deleting older checkpoint [/apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-200] due to args.save_total_limit
 96%|████████████████████████████████████████████████████████████████████▏  | 850/885 [27:09<01:01,  1.75s/it][INFO|trainer.py:4226] 2025-06-26 12:55:16,136 >>
{'loss': 0.8521, 'grad_norm': 0.32779940240689764, 'learning_rate': 4.3490339793756825e-08, 'epoch': 4.58}
{'loss': 0.8244, 'grad_norm': 0.35253482613752385, 'learning_rate': 3.2725570807730976e-08, 'epoch': 4.63}
{'loss': 0.8451, 'grad_norm': 0.3375251482420393, 'learning_rate': 2.3467294212456747e-08, 'epoch': 4.69}
{'loss': 0.8407, 'grad_norm': 0.3110672374261993, 'learning_rate': 1.5729929414486143e-08, 'epoch': 4.75}
{'loss': 0.8225, 'grad_norm': 0.33142812736392335, 'learning_rate': 9.525527059262683e-09, 'epoch': 4.8}
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-06-26 12:55:16,136 >>   Num examples = 90
[INFO|trainer.py:4231] 2025-06-26 12:55:16,136 >>   Batch size = 2
100%|███████████████████████████████████████████████████████████████████████| 885/885 [28:17<00:00,  1.69s/it][INFO|trainer.py:3910] 2025-06-26 12:56:29,220 >> Saving model checkpoint to /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-885
[INFO|configuration_utils.py:694] 2025-06-26 12:56:29,246 >> loading configuration file /apdcephfs_cq11/share_1567347/share_info/llm_models/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json
{'eval_loss': 0.8472508788108826, 'eval_runtime': 6.5157, 'eval_samples_per_second': 13.813, 'eval_steps_per_second': 0.921, 'epoch': 4.8}
{'loss': 0.836, 'grad_norm': 0.2711204597329367, 'learning_rate': 4.863750262708022e-09, 'epoch': 4.86}
{'loss': 0.8297, 'grad_norm': 0.34779413224271233, 'learning_rate': 1.751859561293867e-09, 'epoch': 4.92}
{'loss': 0.8302, 'grad_norm': 0.35427159842935974, 'learning_rate': 1.947016040384497e-10, 'epoch': 4.97}
[INFO|configuration_utils.py:768] 2025-06-26 12:56:29,246 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.3",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2491] 2025-06-26 12:56:29,357 >> tokenizer config file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-885/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-06-26 12:56:29,365 >> Special tokens file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-885/special_tokens_map.json
[2025-06-26 12:56:29,551] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step885 is about to be saved!
[2025-06-26 12:56:29,576] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-885/global_step885/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-06-26 12:56:29,576] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-885/global_step885/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-06-26 12:56:29,647] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-885/global_step885/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-06-26 12:56:29,658] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-885/global_step885/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-06-26 12:56:29,815] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-885/global_step885/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-06-26 12:56:29,825] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-885/global_step885/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-06-26 12:56:29,839] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step885 is ready now!
[INFO|trainer.py:4002] 2025-06-26 12:56:29,937 >> Deleting older checkpoint [/apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/checkpoint-400] due to args.save_total_limit
[INFO|trainer.py:2643] 2025-06-26 12:56:30,310 >>

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|███████████████████████████████████████████████████████████████████████| 885/885 [28:23<00:00,  1.92s/it]
{'train_runtime': 1705.6195, 'train_samples_per_second': 4.133, 'train_steps_per_second': 0.519, 'train_loss': 0.9743304953063275, 'epoch': 5.0}
[INFO|trainer.py:3910] 2025-06-26 12:56:35,706 >> Saving model checkpoint to /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626
[INFO|configuration_utils.py:694] 2025-06-26 12:56:35,724 >> loading configuration file /apdcephfs_cq11/share_1567347/share_info/llm_models/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json
[INFO|configuration_utils.py:768] 2025-06-26 12:56:35,725 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.3",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2491] 2025-06-26 12:56:35,884 >> tokenizer config file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-06-26 12:56:35,905 >> Special tokens file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/special_tokens_map.json
***** train metrics *****
  epoch                    =        5.0
  total_flos               =   636771GF
  train_loss               =     0.9743
  train_runtime            = 0:28:25.61
  train_samples_per_second =      4.133
  train_steps_per_second   =      0.519
Figure saved at: /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/training_loss.png
Figure saved at: /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/qwen2.5-7b_sft_sci_lr2e6_bs8_epoch5_lora_0626/training_eval_loss.png
[WARNING|2025-06-26 12:56:36] llamafactory.extras.ploting:162 >> No metric eval_accuracy to plot.
[INFO|trainer.py:4226] 2025-06-26 12:56:36,422 >>
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-06-26 12:56:36,423 >>   Num examples = 90
[INFO|trainer.py:4231] 2025-06-26 12:56:36,423 >>   Batch size = 2
100%|███████████████████████████████████████████████████████████████████████████| 6/6 [00:05<00:00,  1.07it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_loss               =     0.8474
  eval_runtime            = 0:00:06.53
  eval_samples_per_second =      13.77
  eval_steps_per_second   =      0.918
[INFO|modelcard.py:449] 2025-06-26 12:56:42,998 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
