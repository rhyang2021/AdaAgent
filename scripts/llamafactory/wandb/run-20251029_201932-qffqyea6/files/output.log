  2%|█▏                                                                   | 50/2967 [08:09<7:47:05,  9.61s/it][INFO|trainer.py:4226] 2025-10-29 20:27:43,264 >>
{'loss': 1.2188, 'grad_norm': 5.550542161777127, 'learning_rate': 6.734006734006734e-08, 'epoch': 0.01}
{'loss': 1.2022, 'grad_norm': 5.4097927244431565, 'learning_rate': 1.3468013468013468e-07, 'epoch': 0.02}
{'loss': 1.2322, 'grad_norm': 4.559102857881328, 'learning_rate': 2.02020202020202e-07, 'epoch': 0.03}
{'loss': 1.1568, 'grad_norm': 3.064509016464685, 'learning_rate': 2.6936026936026936e-07, 'epoch': 0.04}
{'loss': 1.1696, 'grad_norm': 2.6003912759906616, 'learning_rate': 3.3670033670033673e-07, 'epoch': 0.05}
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-10-29 20:27:43,265 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-29 20:27:43,265 >>   Batch size = 2
  3%|██▎                                                                 | 100/2967 [18:12<7:37:39,  9.58s/it][INFO|trainer.py:4226] 2025-10-29 20:37:46,221 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 1.0358065366744995, 'eval_runtime': 123.9106, 'eval_samples_per_second': 13.445, 'eval_steps_per_second': 0.847, 'epoch': 0.05}
{'loss': 1.0655, 'grad_norm': 2.1760618693746636, 'learning_rate': 4.04040404040404e-07, 'epoch': 0.06}
{'loss': 1.0672, 'grad_norm': 1.9333880420814413, 'learning_rate': 4.7138047138047136e-07, 'epoch': 0.07}
{'loss': 1.0162, 'grad_norm': 1.8388656255203557, 'learning_rate': 5.387205387205387e-07, 'epoch': 0.08}
{'loss': 0.9879, 'grad_norm': 1.7915969078645544, 'learning_rate': 6.060606060606061e-07, 'epoch': 0.09}
{'loss': 0.9278, 'grad_norm': 1.825712002941217, 'learning_rate': 6.734006734006735e-07, 'epoch': 0.1}
[INFO|trainer.py:4228] 2025-10-29 20:37:46,221 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-29 20:37:46,221 >>   Batch size = 2
  5%|███▍                                                                | 150/2967 [28:17<7:38:05,  9.76s/it][INFO|trainer.py:4226] 2025-10-29 20:47:50,907 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.8929598927497864, 'eval_runtime': 123.9618, 'eval_samples_per_second': 13.44, 'eval_steps_per_second': 0.847, 'epoch': 0.1}
{'loss': 0.939, 'grad_norm': 1.8639863703979789, 'learning_rate': 7.407407407407406e-07, 'epoch': 0.11}
{'loss': 0.9033, 'grad_norm': 1.9231906879580967, 'learning_rate': 8.08080808080808e-07, 'epoch': 0.12}
{'loss': 0.9201, 'grad_norm': 1.7887797688520028, 'learning_rate': 8.754208754208754e-07, 'epoch': 0.13}
{'loss': 0.9252, 'grad_norm': 1.7462561060841972, 'learning_rate': 9.427609427609427e-07, 'epoch': 0.14}
{'loss': 0.9119, 'grad_norm': 1.777497025560163, 'learning_rate': 1.01010101010101e-06, 'epoch': 0.15}
[INFO|trainer.py:4228] 2025-10-29 20:47:50,908 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-29 20:47:50,908 >>   Batch size = 2
  7%|████▌                                                               | 200/2967 [38:18<6:47:36,  8.84s/it][INFO|trainer.py:4226] 2025-10-29 20:57:51,757 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.8517768979072571, 'eval_runtime': 123.9303, 'eval_samples_per_second': 13.443, 'eval_steps_per_second': 0.847, 'epoch': 0.15}
{'loss': 0.8805, 'grad_norm': 1.7665634486589905, 'learning_rate': 1.0774410774410775e-06, 'epoch': 0.16}
{'loss': 0.8926, 'grad_norm': 1.6086885295448738, 'learning_rate': 1.1447811447811446e-06, 'epoch': 0.17}
{'loss': 0.8852, 'grad_norm': 1.7467844680757105, 'learning_rate': 1.2121212121212122e-06, 'epoch': 0.18}
{'loss': 0.8927, 'grad_norm': 1.7684129693633153, 'learning_rate': 1.2794612794612794e-06, 'epoch': 0.19}
{'loss': 0.8551, 'grad_norm': 1.8026531617588506, 'learning_rate': 1.346801346801347e-06, 'epoch': 0.2}
[INFO|trainer.py:4228] 2025-10-29 20:57:51,757 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-29 20:57:51,757 >>   Batch size = 2
  8%|█████▋                                                              | 250/2967 [48:21<7:07:40,  9.44s/it][INFO|trainer.py:4226] 2025-10-29 21:07:54,937 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.8325406908988953, 'eval_runtime': 123.9838, 'eval_samples_per_second': 13.437, 'eval_steps_per_second': 0.847, 'epoch': 0.2}
{'loss': 0.8653, 'grad_norm': 1.734533603164751, 'learning_rate': 1.414141414141414e-06, 'epoch': 0.21}
{'loss': 0.9059, 'grad_norm': 1.4964845476225168, 'learning_rate': 1.4814814814814812e-06, 'epoch': 0.22}
{'loss': 0.8747, 'grad_norm': 1.6399047184244144, 'learning_rate': 1.5488215488215488e-06, 'epoch': 0.23}
{'loss': 0.8789, 'grad_norm': 1.5275383088764625, 'learning_rate': 1.616161616161616e-06, 'epoch': 0.24}
{'loss': 0.8749, 'grad_norm': 1.56954481579364, 'learning_rate': 1.6835016835016836e-06, 'epoch': 0.25}
[INFO|trainer.py:4228] 2025-10-29 21:07:54,937 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-29 21:07:54,937 >>   Batch size = 2
 10%|██████▉                                                             | 300/2967 [58:25<7:03:02,  9.52s/it][INFO|trainer.py:4226] 2025-10-29 21:17:58,814 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.8197361826896667, 'eval_runtime': 123.9993, 'eval_samples_per_second': 13.436, 'eval_steps_per_second': 0.847, 'epoch': 0.25}
{'loss': 0.8467, 'grad_norm': 1.5788545579938829, 'learning_rate': 1.7508417508417507e-06, 'epoch': 0.26}
{'loss': 0.8765, 'grad_norm': 1.4309797686673928, 'learning_rate': 1.818181818181818e-06, 'epoch': 0.27}
{'loss': 0.8585, 'grad_norm': 1.6942636348309692, 'learning_rate': 1.8855218855218854e-06, 'epoch': 0.28}
{'loss': 0.8528, 'grad_norm': 1.5123456302823943, 'learning_rate': 1.9528619528619526e-06, 'epoch': 0.29}
{'loss': 0.8662, 'grad_norm': 1.6003089518525644, 'learning_rate': 1.999993769982229e-06, 'epoch': 0.3}
[INFO|trainer.py:4228] 2025-10-29 21:17:58,814 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-29 21:17:58,814 >>   Batch size = 2
 10%|██████▋                                                           | 300/2967 [1:00:29<7:03:02,  9.52s/it][INFO|trainer.py:3910] 2025-10-29 21:20:08,934 >> Saving model checkpoint to /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-300
[INFO|configuration_utils.py:420] 2025-10-29 21:20:08,953 >> Configuration saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-300/config.json
{'eval_loss': 0.8132717609405518, 'eval_runtime': 124.0064, 'eval_samples_per_second': 13.435, 'eval_steps_per_second': 0.847, 'epoch': 0.3}
[INFO|configuration_utils.py:909] 2025-10-29 21:20:08,962 >> Configuration saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-300/generation_config.json
[INFO|modeling_utils.py:2996] 2025-10-29 21:20:33,743 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-300/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2491] 2025-10-29 21:20:33,754 >> tokenizer config file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-10-29 21:20:33,762 >> Special tokens file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-300/special_tokens_map.json
[2025-10-29 21:20:34,887] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step300 is about to be saved!
[2025-10-29 21:20:34,901] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-300/global_step300/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-10-29 21:20:34,901] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-300/global_step300/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-10-29 21:20:34,933] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-300/global_step300/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-10-29 21:20:34,987] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-300/global_step300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-10-29 21:22:03,998] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-300/global_step300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-10-29 21:22:04,009] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-300/global_step300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-10-29 21:22:05,259] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step300 is ready now!
 12%|███████▊                                                          | 350/2967 [1:10:28<6:43:58,  9.26s/it][INFO|trainer.py:4226] 2025-10-29 21:30:02,074 >>
{'loss': 0.8593, 'grad_norm': 1.3948321064473679, 'learning_rate': 1.999883016270207e-06, 'epoch': 0.31}
{'loss': 0.8577, 'grad_norm': 1.6074645466645012, 'learning_rate': 1.99963383536794e-06, 'epoch': 0.32}
{'loss': 0.8431, 'grad_norm': 1.5273780683442506, 'learning_rate': 1.999246261772876e-06, 'epoch': 0.33}
{'loss': 0.8294, 'grad_norm': 1.6635538063442898, 'learning_rate': 1.9987203491420157e-06, 'epoch': 0.34}
{'loss': 0.8899, 'grad_norm': 1.4570616658880984, 'learning_rate': 1.9980561702844845e-06, 'epoch': 0.35}
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-10-29 21:30:02,074 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-29 21:30:02,074 >>   Batch size = 2
 13%|████████▉                                                         | 400/2967 [1:20:27<6:33:33,  9.20s/it][INFO|trainer.py:4226] 2025-10-29 21:40:00,729 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.8077237010002136, 'eval_runtime': 123.9107, 'eval_samples_per_second': 13.445, 'eval_steps_per_second': 0.847, 'epoch': 0.35}
{'loss': 0.8483, 'grad_norm': 1.4195786733930034, 'learning_rate': 1.997253817151452e-06, 'epoch': 0.36}
{'loss': 0.8501, 'grad_norm': 1.3716644753925966, 'learning_rate': 1.996313400823403e-06, 'epoch': 0.37}
{'loss': 0.8378, 'grad_norm': 1.5304830875592388, 'learning_rate': 1.9952350514947576e-06, 'epoch': 0.38}
{'loss': 0.8288, 'grad_norm': 1.3751726862653357, 'learning_rate': 1.9940189184558483e-06, 'epoch': 0.39}
{'loss': 0.8375, 'grad_norm': 1.4289424114043288, 'learning_rate': 1.992665170072251e-06, 'epoch': 0.4}
[INFO|trainer.py:4228] 2025-10-29 21:40:00,729 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-29 21:40:00,729 >>   Batch size = 2
 15%|██████████                                                        | 450/2967 [1:30:37<6:30:55,  9.32s/it][INFO|trainer.py:4226] 2025-10-29 21:50:10,658 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.802509605884552, 'eval_runtime': 123.9855, 'eval_samples_per_second': 13.437, 'eval_steps_per_second': 0.847, 'epoch': 0.4}
{'loss': 0.837, 'grad_norm': 1.4226890531033674, 'learning_rate': 1.9911739937614747e-06, 'epoch': 0.41}
{'loss': 0.8769, 'grad_norm': 1.428305048656519, 'learning_rate': 1.9895455959670174e-06, 'epoch': 0.42}
{'loss': 0.87, 'grad_norm': 1.4432492114963127, 'learning_rate': 1.987780202129783e-06, 'epoch': 0.43}
{'loss': 0.8562, 'grad_norm': 1.3742820053595435, 'learning_rate': 1.985878056656872e-06, 'epoch': 0.44}
{'loss': 0.8501, 'grad_norm': 1.3755101688274756, 'learning_rate': 1.983839422887744e-06, 'epoch': 0.46}
[INFO|trainer.py:4228] 2025-10-29 21:50:10,659 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-29 21:50:10,659 >>   Batch size = 2
 17%|███████████                                                       | 500/2967 [1:40:38<6:20:13,  9.25s/it][INFO|trainer.py:4226] 2025-10-29 22:00:12,534 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7968822717666626, 'eval_runtime': 123.9997, 'eval_samples_per_second': 13.436, 'eval_steps_per_second': 0.847, 'epoch': 0.46}
{'loss': 0.8526, 'grad_norm': 1.3122391540092644, 'learning_rate': 1.9816645830577604e-06, 'epoch': 0.47}
{'loss': 0.8271, 'grad_norm': 1.4063741852186014, 'learning_rate': 1.9793538382591113e-06, 'epoch': 0.48}
{'loss': 0.8359, 'grad_norm': 1.3363126855248846, 'learning_rate': 1.97690750839913e-06, 'epoch': 0.49}
{'loss': 0.8201, 'grad_norm': 1.3283434297640786, 'learning_rate': 1.974325932156006e-06, 'epoch': 0.5}
{'loss': 0.8534, 'grad_norm': 1.4382909275709481, 'learning_rate': 1.9716094669318944e-06, 'epoch': 0.51}
[INFO|trainer.py:4228] 2025-10-29 22:00:12,534 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-29 22:00:12,534 >>   Batch size = 2
 19%|████████████▏                                                     | 550/2967 [1:50:45<6:24:40,  9.55s/it][INFO|trainer.py:4226] 2025-10-29 22:10:18,910 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7942870855331421, 'eval_runtime': 123.961, 'eval_samples_per_second': 13.44, 'eval_steps_per_second': 0.847, 'epoch': 0.51}
{'loss': 0.8314, 'grad_norm': 1.295428692604964, 'learning_rate': 1.968758488803439e-06, 'epoch': 0.52}
{'loss': 0.853, 'grad_norm': 1.29521182165858, 'learning_rate': 1.9657733924697047e-06, 'epoch': 0.53}
{'loss': 0.8546, 'grad_norm': 1.3724488795048866, 'learning_rate': 1.9626545911975353e-06, 'epoch': 0.54}
{'loss': 0.8443, 'grad_norm': 1.22624138835655, 'learning_rate': 1.9594025167643384e-06, 'epoch': 0.55}
{'loss': 0.8444, 'grad_norm': 1.297030694016761, 'learning_rate': 1.95601761939831e-06, 'epoch': 0.56}
[INFO|trainer.py:4228] 2025-10-29 22:10:18,910 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-29 22:10:18,910 >>   Batch size = 2
 20%|█████████████▎                                                    | 600/2967 [2:00:56<6:10:02,  9.38s/it][INFO|trainer.py:4226] 2025-10-29 22:20:29,756 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.789840817451477, 'eval_runtime': 123.9752, 'eval_samples_per_second': 13.438, 'eval_steps_per_second': 0.847, 'epoch': 0.56}
{'loss': 0.8507, 'grad_norm': 1.3688371142872358, 'learning_rate': 1.9525003677161025e-06, 'epoch': 0.57}
{'loss': 0.8243, 'grad_norm': 1.4253942343336954, 'learning_rate': 1.948851248657947e-06, 'epoch': 0.58}
{'loss': 0.8186, 'grad_norm': 1.2765871133733022, 'learning_rate': 1.9450707674202416e-06, 'epoch': 0.59}
{'loss': 0.8467, 'grad_norm': 1.19813170895132, 'learning_rate': 1.9411594473856097e-06, 'epoch': 0.6}
{'loss': 0.829, 'grad_norm': 1.5080037168905112, 'learning_rate': 1.9371178300504393e-06, 'epoch': 0.61}
[INFO|trainer.py:4228] 2025-10-29 22:20:29,757 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-29 22:20:29,757 >>   Batch size = 2
 20%|█████████████▎                                                    | 600/2967 [2:03:00<6:10:02,  9.38s/it][INFO|trainer.py:3910] 2025-10-29 22:22:39,480 >> Saving model checkpoint to /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-600
[INFO|configuration_utils.py:420] 2025-10-29 22:22:39,506 >> Configuration saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-600/config.json
{'eval_loss': 0.7861059308052063, 'eval_runtime': 123.9453, 'eval_samples_per_second': 13.441, 'eval_steps_per_second': 0.847, 'epoch': 0.61}
[INFO|configuration_utils.py:909] 2025-10-29 22:22:39,515 >> Configuration saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-600/generation_config.json
[INFO|modeling_utils.py:2996] 2025-10-29 22:23:03,541 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-600/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2491] 2025-10-29 22:23:03,553 >> tokenizer config file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-10-29 22:23:03,565 >> Special tokens file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-600/special_tokens_map.json
[2025-10-29 22:23:04,601] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step600 is about to be saved!
[2025-10-29 22:23:04,616] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-600/global_step600/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-10-29 22:23:04,616] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-600/global_step600/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-10-29 22:23:04,678] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-600/global_step600/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-10-29 22:23:04,701] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-10-29 22:24:30,793] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-10-29 22:24:30,803] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-10-29 22:24:32,894] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step600 is ready now!
 22%|██████████████▍                                                   | 650/2967 [2:12:59<6:07:53,  9.53s/it][INFO|trainer.py:4226] 2025-10-29 22:32:33,643 >>
{'loss': 0.8302, 'grad_norm': 1.3215585094029272, 'learning_rate': 1.9329464749499186e-06, 'epoch': 0.62}
{'loss': 0.8345, 'grad_norm': 1.2949296560719061, 'learning_rate': 1.928645959580572e-06, 'epoch': 0.63}
{'loss': 0.8135, 'grad_norm': 1.3628303204790926, 'learning_rate': 1.9242168793203085e-06, 'epoch': 0.64}
{'loss': 0.811, 'grad_norm': 1.2978765479997891, 'learning_rate': 1.9196598473459945e-06, 'epoch': 0.65}
{'loss': 0.848, 'grad_norm': 1.298275584929196, 'learning_rate': 1.9149754945485666e-06, 'epoch': 0.66}
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-10-29 22:32:33,643 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-29 22:32:33,644 >>   Batch size = 2
 24%|███████████████▌                                                  | 700/2967 [2:23:00<6:01:29,  9.57s/it][INFO|trainer.py:4226] 2025-10-29 22:42:34,213 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7843549251556396, 'eval_runtime': 123.9541, 'eval_samples_per_second': 13.44, 'eval_steps_per_second': 0.847, 'epoch': 0.66}
{'loss': 0.807, 'grad_norm': 1.2297197922352765, 'learning_rate': 1.9101644694456873e-06, 'epoch': 0.67}
{'loss': 0.8132, 'grad_norm': 1.3037342363183018, 'learning_rate': 1.90522743809196e-06, 'epoch': 0.68}
{'loss': 0.8388, 'grad_norm': 1.2855083599535586, 'learning_rate': 1.9001650839867223e-06, 'epoch': 0.69}
{'loss': 0.8282, 'grad_norm': 1.2892706834340557, 'learning_rate': 1.894978107979417e-06, 'epoch': 0.7}
{'loss': 0.8326, 'grad_norm': 1.354993753591032, 'learning_rate': 1.8896672281725648e-06, 'epoch': 0.71}
[INFO|trainer.py:4228] 2025-10-29 22:42:34,214 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-29 22:42:34,214 >>   Batch size = 2
 25%|████████████████▋                                                 | 750/2967 [2:33:05<6:01:18,  9.78s/it][INFO|trainer.py:4226] 2025-10-29 22:52:38,702 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7834571599960327, 'eval_runtime': 123.8507, 'eval_samples_per_second': 13.452, 'eval_steps_per_second': 0.848, 'epoch': 0.71}
{'loss': 0.8242, 'grad_norm': 1.2889032874274702, 'learning_rate': 1.8842331798223482e-06, 'epoch': 0.72}
{'loss': 0.8158, 'grad_norm': 1.279245445987933, 'learning_rate': 1.8786767152368194e-06, 'epoch': 0.73}
{'loss': 0.8022, 'grad_norm': 1.3220698899286145, 'learning_rate': 1.8729986036717499e-06, 'epoch': 0.74}
{'loss': 0.834, 'grad_norm': 1.1955069041587847, 'learning_rate': 1.867199631224129e-06, 'epoch': 0.75}
{'loss': 0.8246, 'grad_norm': 1.222345035679992, 'learning_rate': 1.8612806007233382e-06, 'epoch': 0.76}
[INFO|trainer.py:4228] 2025-10-29 22:52:38,702 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-29 22:52:38,702 >>   Batch size = 2
 27%|█████████████████▊                                                | 800/2967 [2:43:08<5:45:58,  9.58s/it][INFO|trainer.py:4226] 2025-10-29 23:02:41,815 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7809716463088989, 'eval_runtime': 123.9614, 'eval_samples_per_second': 13.44, 'eval_steps_per_second': 0.847, 'epoch': 0.76}
{'loss': 0.8193, 'grad_norm': 1.361389496419801, 'learning_rate': 1.85524233162e-06, 'epoch': 0.77}
{'loss': 0.8076, 'grad_norm': 1.375504710843176, 'learning_rate': 1.8490856598725341e-06, 'epoch': 0.78}
{'loss': 0.7912, 'grad_norm': 1.281424283506405, 'learning_rate': 1.8428114378314234e-06, 'epoch': 0.79}
{'loss': 0.8316, 'grad_norm': 1.165575524784986, 'learning_rate': 1.8364205341212105e-06, 'epoch': 0.8}
{'loss': 0.8213, 'grad_norm': 1.192369577672957, 'learning_rate': 1.8299138335202438e-06, 'epoch': 0.81}
[INFO|trainer.py:4228] 2025-10-29 23:02:41,815 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-29 23:02:41,815 >>   Batch size = 2
 29%|██████████████████▉                                               | 850/2967 [2:53:12<5:49:03,  9.89s/it][INFO|trainer.py:4226] 2025-10-29 23:12:46,123 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7785022258758545, 'eval_runtime': 124.0685, 'eval_samples_per_second': 13.428, 'eval_steps_per_second': 0.846, 'epoch': 0.81}
{'loss': 0.8653, 'grad_norm': 1.216745029182527, 'learning_rate': 1.8232922368381857e-06, 'epoch': 0.82}
{'loss': 0.826, 'grad_norm': 1.1544390522262322, 'learning_rate': 1.8165566607913e-06, 'epoch': 0.83}
{'loss': 0.8396, 'grad_norm': 1.174901792385826, 'learning_rate': 1.8097080378755414e-06, 'epoch': 0.84}
{'loss': 0.8051, 'grad_norm': 1.234518428397879, 'learning_rate': 1.802747316237454e-06, 'epoch': 0.85}
{'loss': 0.8358, 'grad_norm': 1.22007455337736, 'learning_rate': 1.79567545954291e-06, 'epoch': 0.86}
[INFO|trainer.py:4228] 2025-10-29 23:12:46,123 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-29 23:12:46,123 >>   Batch size = 2
 30%|████████████████████                                              | 900/2967 [3:03:27<5:37:48,  9.81s/it][INFO|trainer.py:4226] 2025-10-29 23:23:01,520 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7764984369277954, 'eval_runtime': 123.9086, 'eval_samples_per_second': 13.445, 'eval_steps_per_second': 0.847, 'epoch': 0.86}
{'loss': 0.8101, 'grad_norm': 1.2200882294067696, 'learning_rate': 1.7884934468436942e-06, 'epoch': 0.87}
{'loss': 0.8481, 'grad_norm': 1.1946693387460599, 'learning_rate': 1.7812022724419623e-06, 'epoch': 0.88}
{'loss': 0.8472, 'grad_norm': 1.1187853037128634, 'learning_rate': 1.7738029457525854e-06, 'epoch': 0.89}
{'loss': 0.8385, 'grad_norm': 1.2236157728315369, 'learning_rate': 1.7662964911634035e-06, 'epoch': 0.9}
{'loss': 0.8057, 'grad_norm': 1.3227132101525931, 'learning_rate': 1.758683947893406e-06, 'epoch': 0.91}
[INFO|trainer.py:4228] 2025-10-29 23:23:01,520 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-29 23:23:01,520 >>   Batch size = 2
 30%|████████████████████                                              | 900/2967 [3:05:31<5:37:48,  9.81s/it][INFO|trainer.py:3910] 2025-10-29 23:25:11,031 >> Saving model checkpoint to /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-900
[INFO|configuration_utils.py:420] 2025-10-29 23:25:11,057 >> Configuration saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-900/config.json
{'eval_loss': 0.774347186088562, 'eval_runtime': 123.9728, 'eval_samples_per_second': 13.438, 'eval_steps_per_second': 0.847, 'epoch': 0.91}
[INFO|configuration_utils.py:909] 2025-10-29 23:25:11,066 >> Configuration saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-900/generation_config.json
[INFO|modeling_utils.py:2996] 2025-10-29 23:25:27,141 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-900/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2491] 2025-10-29 23:25:27,152 >> tokenizer config file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-10-29 23:25:27,160 >> Special tokens file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-900/special_tokens_map.json
[2025-10-29 23:25:28,092] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step900 is about to be saved!
[2025-10-29 23:25:28,106] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-900/global_step900/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-10-29 23:25:28,106] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-900/global_step900/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-10-29 23:25:28,129] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-900/global_step900/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-10-29 23:25:28,191] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-900/global_step900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-10-29 23:26:15,290] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-900/global_step900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-10-29 23:26:15,301] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-900/global_step900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-10-29 23:26:16,770] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step900 is ready now!
 32%|█████████████████████▏                                            | 950/2967 [3:14:37<5:27:50,  9.75s/it][INFO|trainer.py:4226] 2025-10-29 23:34:11,567 >>
{'loss': 0.8232, 'grad_norm': 1.194045783800643, 'learning_rate': 1.7509663698488578e-06, 'epoch': 0.92}
{'loss': 0.8375, 'grad_norm': 1.1985197740568039, 'learning_rate': 1.743144825477394e-06, 'epoch': 0.93}
{'loss': 0.8268, 'grad_norm': 1.314227726096899, 'learning_rate': 1.735220397620101e-06, 'epoch': 0.94}
{'loss': 0.7723, 'grad_norm': 1.3010398699806833, 'learning_rate': 1.7271941833616017e-06, 'epoch': 0.95}
{'loss': 0.8394, 'grad_norm': 1.154845081897965, 'learning_rate': 1.719067293878174e-06, 'epoch': 0.96}
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-10-29 23:34:11,567 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-29 23:34:11,567 >>   Batch size = 2
 34%|█████████████████████▉                                           | 1000/2967 [3:24:47<5:18:31,  9.72s/it][INFO|trainer.py:4226] 2025-10-29 23:44:20,849 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7737012505531311, 'eval_runtime': 123.9438, 'eval_samples_per_second': 13.442, 'eval_steps_per_second': 0.847, 'epoch': 0.96}
{'loss': 0.8219, 'grad_norm': 1.1670738388758823, 'learning_rate': 1.7108408542839144e-06, 'epoch': 0.97}
{'loss': 0.7986, 'grad_norm': 1.1761725843322266, 'learning_rate': 1.702516003474974e-06, 'epoch': 0.98}
{'loss': 0.8052, 'grad_norm': 1.1365328653613644, 'learning_rate': 1.6940938939718858e-06, 'epoch': 0.99}
[2025-10-29 23:41:52,067] [WARNING] [stage3.py:2114:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8334, 'grad_norm': 1.1006063417839476, 'learning_rate': 1.6855756917600054e-06, 'epoch': 1.0}
{'loss': 0.7793, 'grad_norm': 1.3069408691695041, 'learning_rate': 1.6769625761280887e-06, 'epoch': 1.01}
[INFO|trainer.py:4228] 2025-10-29 23:44:20,850 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-29 23:44:20,850 >>   Batch size = 2
 35%|███████████████████████                                          | 1050/2967 [3:34:55<5:22:52, 10.11s/it][INFO|trainer.py:4226] 2025-10-29 23:54:28,804 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7749688029289246, 'eval_runtime': 123.9342, 'eval_samples_per_second': 13.443, 'eval_steps_per_second': 0.847, 'epoch': 1.01}
{'loss': 0.76, 'grad_norm': 1.252093582872227, 'learning_rate': 1.6682557395050265e-06, 'epoch': 1.02}
{'loss': 0.7599, 'grad_norm': 1.2607836907181558, 'learning_rate': 1.6594563872947611e-06, 'epoch': 1.03}
{'loss': 0.7586, 'grad_norm': 1.2800450181885137, 'learning_rate': 1.6505657377094062e-06, 'epoch': 1.04}
{'loss': 0.7656, 'grad_norm': 1.2024873671195155, 'learning_rate': 1.6415850216005926e-06, 'epoch': 1.05}
{'loss': 0.8029, 'grad_norm': 1.3398549061297704, 'learning_rate': 1.6325154822890662e-06, 'epoch': 1.06}
[INFO|trainer.py:4228] 2025-10-29 23:54:28,804 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-29 23:54:28,804 >>   Batch size = 2
 37%|████████████████████████                                         | 1100/2967 [3:45:01<5:05:13,  9.81s/it][INFO|trainer.py:4226] 2025-10-30 00:04:35,220 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7746338844299316, 'eval_runtime': 123.966, 'eval_samples_per_second': 13.439, 'eval_steps_per_second': 0.847, 'epoch': 1.06}
{'loss': 0.7488, 'grad_norm': 1.248326403790969, 'learning_rate': 1.6233583753925575e-06, 'epoch': 1.07}
{'loss': 0.7792, 'grad_norm': 1.2350078335791623, 'learning_rate': 1.6141149686519498e-06, 'epoch': 1.08}
{'loss': 0.7607, 'grad_norm': 1.121119719553074, 'learning_rate': 1.6047865417557692e-06, 'epoch': 1.09}
{'loss': 0.7352, 'grad_norm': 1.149849301794544, 'learning_rate': 1.5953743861630206e-06, 'epoch': 1.1}
{'loss': 0.7742, 'grad_norm': 1.1695044356027027, 'learning_rate': 1.5858798049243922e-06, 'epoch': 1.11}
[INFO|trainer.py:4228] 2025-10-30 00:04:35,220 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 00:04:35,220 >>   Batch size = 2
 39%|█████████████████████████▏                                       | 1150/2967 [3:55:10<4:52:13,  9.65s/it][INFO|trainer.py:4226] 2025-10-30 00:14:44,582 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7722502946853638, 'eval_runtime': 123.9877, 'eval_samples_per_second': 13.437, 'eval_steps_per_second': 0.847, 'epoch': 1.11}
{'loss': 0.783, 'grad_norm': 1.2336953760530407, 'learning_rate': 1.5763041125018578e-06, 'epoch': 1.12}
{'loss': 0.7689, 'grad_norm': 1.3828321123855671, 'learning_rate': 1.5666486345866988e-06, 'epoch': 1.13}
{'loss': 0.7724, 'grad_norm': 1.308514754021721, 'learning_rate': 1.5569147079159702e-06, 'epoch': 1.14}
{'loss': 0.7592, 'grad_norm': 1.3074946547206876, 'learning_rate': 1.5471036800874373e-06, 'epoch': 1.15}
{'loss': 0.7792, 'grad_norm': 1.3170825087433509, 'learning_rate': 1.537216909373012e-06, 'epoch': 1.16}
[INFO|trainer.py:4228] 2025-10-30 00:14:44,582 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 00:14:44,582 >>   Batch size = 2
 40%|██████████████████████████▎                                      | 1200/2967 [4:05:15<4:46:47,  9.74s/it][INFO|trainer.py:4226] 2025-10-30 00:24:49,180 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7736820578575134, 'eval_runtime': 124.1242, 'eval_samples_per_second': 13.422, 'eval_steps_per_second': 0.846, 'epoch': 1.16}
{'loss': 0.7798, 'grad_norm': 1.2090618367174062, 'learning_rate': 1.5272557645307075e-06, 'epoch': 1.17}
{'loss': 0.7652, 'grad_norm': 1.1661281113615076, 'learning_rate': 1.5172216246151424e-06, 'epoch': 1.18}
{'loss': 0.7797, 'grad_norm': 1.213888582730073, 'learning_rate': 1.5071158787866214e-06, 'epoch': 1.19}
{'loss': 0.7893, 'grad_norm': 1.276837706781456, 'learning_rate': 1.4969399261188135e-06, 'epoch': 1.2}
{'loss': 0.758, 'grad_norm': 1.2271927378332046, 'learning_rate': 1.4866951754050603e-06, 'epoch': 1.21}
[INFO|trainer.py:4228] 2025-10-30 00:24:49,180 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 00:24:49,180 >>   Batch size = 2
 40%|██████████████████████████▎                                      | 1200/2967 [4:07:19<4:46:47,  9.74s/it][INFO|trainer.py:3910] 2025-10-30 00:26:58,672 >> Saving model checkpoint to /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1200
[INFO|configuration_utils.py:420] 2025-10-30 00:26:58,698 >> Configuration saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1200/config.json
{'eval_loss': 0.7711797952651978, 'eval_runtime': 123.9696, 'eval_samples_per_second': 13.439, 'eval_steps_per_second': 0.847, 'epoch': 1.21}
[INFO|configuration_utils.py:909] 2025-10-30 00:26:58,708 >> Configuration saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1200/generation_config.json
[INFO|modeling_utils.py:2996] 2025-10-30 00:27:14,594 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1200/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2491] 2025-10-30 00:27:14,605 >> tokenizer config file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-10-30 00:27:14,613 >> Special tokens file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1200/special_tokens_map.json
[2025-10-30 00:27:15,514] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step1200 is about to be saved!
[2025-10-30 00:27:15,528] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1200/global_step1200/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-10-30 00:27:15,528] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1200/global_step1200/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-10-30 00:27:15,551] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1200/global_step1200/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-10-30 00:27:15,612] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1200/global_step1200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-10-30 00:28:02,797] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1200/global_step1200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-10-30 00:28:02,808] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1200/global_step1200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-10-30 00:28:03,985] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1200 is ready now!
 42%|███████████████████████████▍                                     | 1250/2967 [4:16:27<4:30:13,  9.44s/it][INFO|trainer.py:4226] 2025-10-30 00:36:01,333 >>
{'loss': 0.7383, 'grad_norm': 1.311358717485806, 'learning_rate': 1.476383044963338e-06, 'epoch': 1.22}
{'loss': 0.7471, 'grad_norm': 1.2711009022190654, 'learning_rate': 1.466004962439901e-06, 'epoch': 1.23}
{'loss': 0.7385, 'grad_norm': 1.3005443659896325, 'learning_rate': 1.4555623646116325e-06, 'epoch': 1.24}
{'loss': 0.7553, 'grad_norm': 1.1831595914239539, 'learning_rate': 1.4450566971871323e-06, 'epoch': 1.25}
{'loss': 0.7751, 'grad_norm': 1.2540468258034458, 'learning_rate': 1.4344894146065704e-06, 'epoch': 1.26}
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-10-30 00:36:01,333 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 00:36:01,333 >>   Batch size = 2
 44%|████████████████████████████▍                                    | 1300/2967 [4:26:26<4:27:07,  9.61s/it][INFO|trainer.py:4226] 2025-10-30 00:45:59,757 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7714306116104126, 'eval_runtime': 123.991, 'eval_samples_per_second': 13.436, 'eval_steps_per_second': 0.847, 'epoch': 1.26}
{'loss': 0.7522, 'grad_norm': 1.3767042999428614, 'learning_rate': 1.4238619798403264e-06, 'epoch': 1.27}
{'loss': 0.7715, 'grad_norm': 1.2985622743770708, 'learning_rate': 1.413175864186452e-06, 'epoch': 1.28}
{'loss': 0.7449, 'grad_norm': 1.2012404751354044, 'learning_rate': 1.402432547066981e-06, 'epoch': 1.29}
{'loss': 0.7833, 'grad_norm': 1.2191419182291625, 'learning_rate': 1.3916335158231098e-06, 'epoch': 1.3}
{'loss': 0.7808, 'grad_norm': 1.3457728357002643, 'learning_rate': 1.3807802655092888e-06, 'epoch': 1.31}
[INFO|trainer.py:4228] 2025-10-30 00:45:59,757 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 00:45:59,758 >>   Batch size = 2
 46%|█████████████████████████████▌                                   | 1350/2967 [4:36:34<4:16:20,  9.51s/it][INFO|trainer.py:4226] 2025-10-30 00:56:07,758 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7708171010017395, 'eval_runtime': 123.9101, 'eval_samples_per_second': 13.445, 'eval_steps_per_second': 0.847, 'epoch': 1.31}
{'loss': 0.7621, 'grad_norm': 1.2191198498760858, 'learning_rate': 1.3698742986862385e-06, 'epoch': 1.32}
{'loss': 0.7697, 'grad_norm': 1.2606298937913911, 'learning_rate': 1.3589171252129329e-06, 'epoch': 1.33}
{'loss': 0.7363, 'grad_norm': 1.308064699782814, 'learning_rate': 1.3479102620375661e-06, 'epoch': 1.34}
{'loss': 0.7721, 'grad_norm': 1.2103982383293663, 'learning_rate': 1.3368552329875446e-06, 'epoch': 1.35}
{'loss': 0.7635, 'grad_norm': 1.2748496887402387, 'learning_rate': 1.3257535685585208e-06, 'epoch': 1.37}
[INFO|trainer.py:4228] 2025-10-30 00:56:07,758 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 00:56:07,758 >>   Batch size = 2
 47%|██████████████████████████████▋                                  | 1400/2967 [4:46:41<4:07:12,  9.47s/it][INFO|trainer.py:4226] 2025-10-30 01:06:14,780 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.76851886510849, 'eval_runtime': 123.9694, 'eval_samples_per_second': 13.439, 'eval_steps_per_second': 0.847, 'epoch': 1.37}
{'loss': 0.7611, 'grad_norm': 1.254928920042054, 'learning_rate': 1.314606805702507e-06, 'epoch': 1.38}
{'loss': 0.7845, 'grad_norm': 1.4240837083614137, 'learning_rate': 1.3034164876150944e-06, 'epoch': 1.39}
{'loss': 0.7811, 'grad_norm': 1.3116651797062164, 'learning_rate': 1.292184163521808e-06, 'epoch': 1.4}
{'loss': 0.7747, 'grad_norm': 1.171852739762025, 'learning_rate': 1.2809113884636268e-06, 'epoch': 1.41}
{'loss': 0.7667, 'grad_norm': 1.2378328370670963, 'learning_rate': 1.2695997230816972e-06, 'epoch': 1.42}
[INFO|trainer.py:4228] 2025-10-30 01:06:14,780 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 01:06:14,780 >>   Batch size = 2
 49%|███████████████████████████████▊                                 | 1450/2967 [4:56:40<4:07:54,  9.81s/it][INFO|trainer.py:4226] 2025-10-30 01:16:14,107 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7682263255119324, 'eval_runtime': 123.9713, 'eval_samples_per_second': 13.439, 'eval_steps_per_second': 0.847, 'epoch': 1.42}
{'loss': 0.763, 'grad_norm': 1.1515123631220898, 'learning_rate': 1.2582507334012753e-06, 'epoch': 1.43}
{'loss': 0.7723, 'grad_norm': 1.2534588865617406, 'learning_rate': 1.2468659906149181e-06, 'epoch': 1.44}
{'loss': 0.7618, 'grad_norm': 1.2758576734883893, 'learning_rate': 1.2354470708649646e-06, 'epoch': 1.45}
{'loss': 0.7569, 'grad_norm': 1.2470363364069357, 'learning_rate': 1.2239955550253277e-06, 'epoch': 1.46}
{'loss': 0.7254, 'grad_norm': 1.239211358358052, 'learning_rate': 1.2125130284826337e-06, 'epoch': 1.47}
[INFO|trainer.py:4228] 2025-10-30 01:16:14,108 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 01:16:14,108 >>   Batch size = 2
 51%|████████████████████████████████▊                                | 1500/2967 [5:06:46<3:48:38,  9.35s/it][INFO|trainer.py:4226] 2025-10-30 01:26:20,478 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7676412463188171, 'eval_runtime': 123.9626, 'eval_samples_per_second': 13.44, 'eval_steps_per_second': 0.847, 'epoch': 1.47}
{'loss': 0.7764, 'grad_norm': 1.2991321700713108, 'learning_rate': 1.201001080916735e-06, 'epoch': 1.48}
{'loss': 0.7799, 'grad_norm': 1.3484713680662064, 'learning_rate': 1.189461306080631e-06, 'epoch': 1.49}
{'loss': 0.7604, 'grad_norm': 1.2380469329257198, 'learning_rate': 1.1778953015798226e-06, 'epoch': 1.5}
{'loss': 0.7823, 'grad_norm': 1.1621057125879068, 'learning_rate': 1.1663046686511328e-06, 'epoch': 1.51}
{'loss': 0.756, 'grad_norm': 1.1992815647634847, 'learning_rate': 1.1546910119410295e-06, 'epoch': 1.52}
[INFO|trainer.py:4228] 2025-10-30 01:26:20,479 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 01:26:20,479 >>   Batch size = 2
 51%|████████████████████████████████▊                                | 1500/2967 [5:08:50<3:48:38,  9.35s/it][INFO|trainer.py:3910] 2025-10-30 01:28:29,764 >> Saving model checkpoint to /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1500
[INFO|configuration_utils.py:420] 2025-10-30 01:28:29,790 >> Configuration saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1500/config.json
{'eval_loss': 0.7653419971466064, 'eval_runtime': 123.8845, 'eval_samples_per_second': 13.448, 'eval_steps_per_second': 0.848, 'epoch': 1.52}
[INFO|configuration_utils.py:909] 2025-10-30 01:28:29,803 >> Configuration saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1500/generation_config.json
[INFO|modeling_utils.py:2996] 2025-10-30 01:28:45,431 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1500/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2491] 2025-10-30 01:28:45,443 >> tokenizer config file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-10-30 01:28:45,451 >> Special tokens file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1500/special_tokens_map.json
[2025-10-30 01:28:45,603] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step1500 is about to be saved!
[2025-10-30 01:28:46,320] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1500/global_step1500/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-10-30 01:28:46,320] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1500/global_step1500/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-10-30 01:28:46,338] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1500/global_step1500/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-10-30 01:28:46,350] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1500/global_step1500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-10-30 01:29:32,493] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1500/global_step1500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-10-30 01:29:32,505] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1500/global_step1500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-10-30 01:29:33,968] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1500 is ready now!
 52%|█████████████████████████████████▉                               | 1550/2967 [5:18:00<3:49:07,  9.70s/it][INFO|trainer.py:4226] 2025-10-30 01:37:34,036 >>
{'loss': 0.7658, 'grad_norm': 1.3114841273076514, 'learning_rate': 1.1430559392834698e-06, 'epoch': 1.53}
{'loss': 0.7828, 'grad_norm': 1.210170313159541, 'learning_rate': 1.131401061477307e-06, 'epoch': 1.54}
{'loss': 0.7857, 'grad_norm': 1.1958615601046356, 'learning_rate': 1.1197279920632862e-06, 'epoch': 1.55}
{'loss': 0.767, 'grad_norm': 1.2742605697789557, 'learning_rate': 1.1080383471006613e-06, 'epoch': 1.56}
{'loss': 0.7936, 'grad_norm': 1.3049161782211711, 'learning_rate': 1.0963337449434602e-06, 'epoch': 1.57}
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-10-30 01:37:34,036 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 01:37:34,036 >>   Batch size = 2
 54%|███████████████████████████████████                              | 1600/2967 [5:28:05<3:43:37,  9.82s/it][INFO|trainer.py:4226] 2025-10-30 01:47:39,553 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7649837136268616, 'eval_runtime': 123.9001, 'eval_samples_per_second': 13.446, 'eval_steps_per_second': 0.847, 'epoch': 1.57}
{'loss': 0.7777, 'grad_norm': 1.3170678234740891, 'learning_rate': 1.0846158060164357e-06, 'epoch': 1.58}
{'loss': 0.7483, 'grad_norm': 1.2196223504587158, 'learning_rate': 1.072886152590728e-06, 'epoch': 1.59}
{'loss': 0.7681, 'grad_norm': 1.2896841962711543, 'learning_rate': 1.0611464085592729e-06, 'epoch': 1.6}
{'loss': 0.766, 'grad_norm': 1.2147707688574565, 'learning_rate': 1.0493981992119831e-06, 'epoch': 1.61}
{'loss': 0.8117, 'grad_norm': 1.2431269032152337, 'learning_rate': 1.0376431510107386e-06, 'epoch': 1.62}
[INFO|trainer.py:4228] 2025-10-30 01:47:39,553 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 01:47:39,553 >>   Batch size = 2
 56%|████████████████████████████████████▏                            | 1650/2967 [5:38:12<3:34:06,  9.75s/it][INFO|trainer.py:4226] 2025-10-30 01:57:46,647 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.763207733631134, 'eval_runtime': 123.9864, 'eval_samples_per_second': 13.437, 'eval_steps_per_second': 0.847, 'epoch': 1.62}
{'loss': 0.7223, 'grad_norm': 1.151935998642678, 'learning_rate': 1.0258828913642137e-06, 'epoch': 1.63}
{'loss': 0.7521, 'grad_norm': 1.2676559095340882, 'learning_rate': 1.0141190484025721e-06, 'epoch': 1.64}
{'loss': 0.771, 'grad_norm': 1.2712665704284536, 'learning_rate': 1.0023532507520642e-06, 'epoch': 1.65}
{'loss': 0.7528, 'grad_norm': 1.2107635418962035, 'learning_rate': 9.905871273095544e-07, 'epoch': 1.66}
{'loss': 0.7551, 'grad_norm': 1.250165593089497, 'learning_rate': 9.788223070170112e-07, 'epoch': 1.67}
[INFO|trainer.py:4228] 2025-10-30 01:57:46,647 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 01:57:46,647 >>   Batch size = 2
 57%|█████████████████████████████████████▏                           | 1700/2967 [5:48:14<3:17:03,  9.33s/it][INFO|trainer.py:4226] 2025-10-30 02:07:47,723 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7627882957458496, 'eval_runtime': 123.9184, 'eval_samples_per_second': 13.444, 'eval_steps_per_second': 0.847, 'epoch': 1.67}
{'loss': 0.7515, 'grad_norm': 1.2057025286277487, 'learning_rate': 9.670604186359888e-07, 'epoch': 1.68}
{'loss': 0.7737, 'grad_norm': 1.466012948349679, 'learning_rate': 9.553030905221395e-07, 'epoch': 1.69}
{'loss': 0.7614, 'grad_norm': 1.267745505208214, 'learning_rate': 9.435519503997765e-07, 'epoch': 1.7}
{'loss': 0.7726, 'grad_norm': 1.2656977233420308, 'learning_rate': 9.318086251365257e-07, 'epoch': 1.71}
{'loss': 0.7531, 'grad_norm': 1.2366759492091985, 'learning_rate': 9.20074740518098e-07, 'epoch': 1.72}
[INFO|trainer.py:4228] 2025-10-30 02:07:47,723 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 02:07:47,723 >>   Batch size = 2
 59%|██████████████████████████████████████▎                          | 1750/2967 [5:58:21<3:12:29,  9.49s/it][INFO|trainer.py:4226] 2025-10-30 02:17:55,443 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7614112496376038, 'eval_runtime': 124.0459, 'eval_samples_per_second': 13.431, 'eval_steps_per_second': 0.846, 'epoch': 1.72}
{'loss': 0.7753, 'grad_norm': 1.31270133016209, 'learning_rate': 9.083519210232095e-07, 'epoch': 1.73}
{'loss': 0.7992, 'grad_norm': 1.3022237065695272, 'learning_rate': 8.966417895986827e-07, 'epoch': 1.74}
{'loss': 0.757, 'grad_norm': 1.3818058651882097, 'learning_rate': 8.849459674347611e-07, 'epoch': 1.75}
{'loss': 0.7867, 'grad_norm': 1.1782497259731466, 'learning_rate': 8.732660737406661e-07, 'epoch': 1.76}
{'loss': 0.7744, 'grad_norm': 1.3295456863171589, 'learning_rate': 8.616037255204265e-07, 'epoch': 1.77}
[INFO|trainer.py:4228] 2025-10-30 02:17:55,443 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 02:17:55,443 >>   Batch size = 2
 61%|███████████████████████████████████████▍                         | 1800/2967 [6:08:19<3:07:20,  9.63s/it][INFO|trainer.py:4226] 2025-10-30 02:27:53,222 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7608188986778259, 'eval_runtime': 123.9542, 'eval_samples_per_second': 13.44, 'eval_steps_per_second': 0.847, 'epoch': 1.77}
{'loss': 0.7865, 'grad_norm': 1.2418072545655414, 'learning_rate': 8.499605373490174e-07, 'epoch': 1.78}
{'loss': 0.7242, 'grad_norm': 1.2241739551581965, 'learning_rate': 8.38338121148832e-07, 'epoch': 1.79}
{'loss': 0.7734, 'grad_norm': 1.3213438833012263, 'learning_rate': 8.267380859665205e-07, 'epoch': 1.8}
{'loss': 0.7654, 'grad_norm': 1.2504227088047764, 'learning_rate': 8.151620377502297e-07, 'epoch': 1.81}
{'loss': 0.7394, 'grad_norm': 1.0756348653993755, 'learning_rate': 8.036115791272697e-07, 'epoch': 1.82}
[INFO|trainer.py:4228] 2025-10-30 02:27:53,222 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 02:27:53,222 >>   Batch size = 2
 61%|███████████████████████████████████████▍                         | 1800/2967 [6:10:23<3:07:20,  9.63s/it][INFO|trainer.py:3910] 2025-10-30 02:30:02,368 >> Saving model checkpoint to /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1800
[INFO|configuration_utils.py:420] 2025-10-30 02:30:02,395 >> Configuration saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1800/config.json
{'eval_loss': 0.7603383660316467, 'eval_runtime': 123.915, 'eval_samples_per_second': 13.445, 'eval_steps_per_second': 0.847, 'epoch': 1.82}
[INFO|configuration_utils.py:909] 2025-10-30 02:30:02,405 >> Configuration saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1800/generation_config.json
[INFO|modeling_utils.py:2996] 2025-10-30 02:30:18,014 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1800/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2491] 2025-10-30 02:30:18,025 >> tokenizer config file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-10-30 02:30:18,033 >> Special tokens file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1800/special_tokens_map.json
[2025-10-30 02:30:18,818] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step1800 is about to be saved!
[2025-10-30 02:30:18,833] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1800/global_step1800/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-10-30 02:30:18,833] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1800/global_step1800/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-10-30 02:30:18,878] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1800/global_step1800/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-10-30 02:30:18,918] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1800/global_step1800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-10-30 02:31:05,134] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1800/global_step1800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-10-30 02:31:05,146] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-1800/global_step1800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-10-30 02:31:07,481] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1800 is ready now!
 62%|████████████████████████████████████████▌                        | 1850/2967 [6:19:34<2:52:31,  9.27s/it][INFO|trainer.py:4226] 2025-10-30 02:39:07,833 >>
{'loss': 0.7821, 'grad_norm': 1.2385260121794788, 'learning_rate': 7.920883091822408e-07, 'epoch': 1.83}
{'loss': 0.7559, 'grad_norm': 1.2720349818344934, 'learning_rate': 7.805938232356503e-07, 'epoch': 1.84}
{'loss': 0.7723, 'grad_norm': 1.1467113220841803, 'learning_rate': 7.691297126230515e-07, 'epoch': 1.85}
{'loss': 0.7745, 'grad_norm': 1.2075913565847696, 'learning_rate': 7.576975644747337e-07, 'epoch': 1.86}
{'loss': 0.7378, 'grad_norm': 1.3142870748822753, 'learning_rate': 7.462989614959941e-07, 'epoch': 1.87}
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-10-30 02:39:07,833 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 02:39:07,833 >>   Batch size = 2
 64%|█████████████████████████████████████████▌                       | 1900/2967 [6:29:33<2:53:01,  9.73s/it][INFO|trainer.py:4226] 2025-10-30 02:49:06,680 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7604787349700928, 'eval_runtime': 123.9701, 'eval_samples_per_second': 13.439, 'eval_steps_per_second': 0.847, 'epoch': 1.87}
{'loss': 0.7523, 'grad_norm': 1.4114992593066182, 'learning_rate': 7.349354817480234e-07, 'epoch': 1.88}
{'loss': 0.7497, 'grad_norm': 1.2959749412930412, 'learning_rate': 7.236086984294332e-07, 'epoch': 1.89}
{'loss': 0.7577, 'grad_norm': 1.2546457664791166, 'learning_rate': 7.123201796584569e-07, 'epoch': 1.9}
{'loss': 0.7521, 'grad_norm': 1.2384213051521584, 'learning_rate': 7.01071488255855e-07, 'epoch': 1.91}
{'loss': 0.7531, 'grad_norm': 1.3980774737241308, 'learning_rate': 6.898641815285515e-07, 'epoch': 1.92}
[INFO|trainer.py:4228] 2025-10-30 02:49:06,680 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 02:49:06,680 >>   Batch size = 2
 66%|██████████████████████████████████████████▋                      | 1950/2967 [6:39:32<2:40:35,  9.47s/it][INFO|trainer.py:4226] 2025-10-30 02:59:05,781 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7594488859176636, 'eval_runtime': 124.0033, 'eval_samples_per_second': 13.435, 'eval_steps_per_second': 0.847, 'epoch': 1.92}
{'loss': 0.7716, 'grad_norm': 1.2460961566399076, 'learning_rate': 6.786998110540363e-07, 'epoch': 1.93}
{'loss': 0.7774, 'grad_norm': 1.2421306786278163, 'learning_rate': 6.675799224655587e-07, 'epoch': 1.94}
{'loss': 0.7692, 'grad_norm': 1.3436786047199338, 'learning_rate': 6.565060552381456e-07, 'epoch': 1.95}
{'loss': 0.749, 'grad_norm': 1.1705309801511217, 'learning_rate': 6.45479742475471e-07, 'epoch': 1.96}
{'loss': 0.7835, 'grad_norm': 1.337129264411596, 'learning_rate': 6.345025106976069e-07, 'epoch': 1.97}
[INFO|trainer.py:4228] 2025-10-30 02:59:05,781 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 02:59:05,781 >>   Batch size = 2
 67%|███████████████████████████████████████████▊                     | 2000/2967 [6:49:32<2:38:21,  9.83s/it][INFO|trainer.py:4226] 2025-10-30 03:09:06,152 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7575677633285522, 'eval_runtime': 123.9144, 'eval_samples_per_second': 13.445, 'eval_steps_per_second': 0.847, 'epoch': 1.97}
{'loss': 0.7279, 'grad_norm': 1.2430488257430017, 'learning_rate': 6.235758796296881e-07, 'epoch': 1.98}
{'loss': 0.7785, 'grad_norm': 1.3328921385580508, 'learning_rate': 6.127013619915164e-07, 'epoch': 1.99}
{'loss': 0.73, 'grad_norm': 1.3980579030582951, 'learning_rate': 6.018804632881324e-07, 'epoch': 2.0}
{'loss': 0.7416, 'grad_norm': 1.3724290821205376, 'learning_rate': 5.911146816013913e-07, 'epoch': 2.01}
{'loss': 0.704, 'grad_norm': 1.3074964727042595, 'learning_rate': 5.804055073825613e-07, 'epoch': 2.02}
[INFO|trainer.py:4228] 2025-10-30 03:09:06,152 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 03:09:06,152 >>   Batch size = 2
 69%|████████████████████████████████████████████▉                    | 2050/2967 [6:59:40<2:28:21,  9.71s/it][INFO|trainer.py:4226] 2025-10-30 03:19:13,776 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7691822052001953, 'eval_runtime': 124.0137, 'eval_samples_per_second': 13.434, 'eval_steps_per_second': 0.847, 'epoch': 2.02}
{'loss': 0.6996, 'grad_norm': 1.7936306596544322, 'learning_rate': 5.697544232459815e-07, 'epoch': 2.03}
{'loss': 0.7166, 'grad_norm': 1.2716184219259405, 'learning_rate': 5.591629037638017e-07, 'epoch': 2.04}
{'loss': 0.7127, 'grad_norm': 1.3355413334036472, 'learning_rate': 5.486324152618418e-07, 'epoch': 2.05}
{'loss': 0.716, 'grad_norm': 1.314101989575856, 'learning_rate': 5.381644156165844e-07, 'epoch': 2.06}
{'loss': 0.6843, 'grad_norm': 1.328430640433942, 'learning_rate': 5.277603540533429e-07, 'epoch': 2.07}
[INFO|trainer.py:4228] 2025-10-30 03:19:13,776 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 03:19:13,776 >>   Batch size = 2
 71%|██████████████████████████████████████████████                   | 2100/2967 [7:09:44<2:18:19,  9.57s/it][INFO|trainer.py:4226] 2025-10-30 03:29:18,056 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7702056765556335, 'eval_runtime': 124.0128, 'eval_samples_per_second': 13.434, 'eval_steps_per_second': 0.847, 'epoch': 2.07}
{'loss': 0.7052, 'grad_norm': 1.2495285269572813, 'learning_rate': 5.174216709456283e-07, 'epoch': 2.08}
{'loss': 0.7001, 'grad_norm': 1.3912101134997668, 'learning_rate': 5.071497976157345e-07, 'epoch': 2.09}
{'loss': 0.7244, 'grad_norm': 1.352004827976054, 'learning_rate': 4.969461561365845e-07, 'epoch': 2.1}
{'loss': 0.7195, 'grad_norm': 1.3775104415250643, 'learning_rate': 4.868121591348535e-07, 'epoch': 2.11}
{'loss': 0.6938, 'grad_norm': 1.341156895899248, 'learning_rate': 4.7674920959539843e-07, 'epoch': 2.12}
[INFO|trainer.py:4228] 2025-10-30 03:29:18,056 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 03:29:18,056 >>   Batch size = 2
 71%|██████████████████████████████████████████████                   | 2100/2967 [7:11:48<2:18:19,  9.57s/it][INFO|trainer.py:3910] 2025-10-30 03:31:27,110 >> Saving model checkpoint to /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2100
[INFO|configuration_utils.py:420] 2025-10-30 03:31:27,137 >> Configuration saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2100/config.json
{'eval_loss': 0.7680384516716003, 'eval_runtime': 123.9521, 'eval_samples_per_second': 13.441, 'eval_steps_per_second': 0.847, 'epoch': 2.12}
[INFO|configuration_utils.py:909] 2025-10-30 03:31:27,146 >> Configuration saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2100/generation_config.json
[INFO|modeling_utils.py:2996] 2025-10-30 03:31:42,937 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2100/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2491] 2025-10-30 03:31:42,949 >> tokenizer config file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-10-30 03:31:42,961 >> Special tokens file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2100/special_tokens_map.json
[2025-10-30 03:31:43,700] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step2100 is about to be saved!
[2025-10-30 03:31:43,714] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2100/global_step2100/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-10-30 03:31:43,715] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2100/global_step2100/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-10-30 03:31:43,739] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2100/global_step2100/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-10-30 03:31:43,801] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2100/global_step2100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-10-30 03:32:31,210] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2100/global_step2100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-10-30 03:32:31,220] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2100/global_step2100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-10-30 03:32:31,412] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2100 is ready now!
 72%|███████████████████████████████████████████████                  | 2150/2967 [7:20:57<2:13:49,  9.83s/it][INFO|trainer.py:4226] 2025-10-30 03:40:30,873 >>
{'loss': 0.7091, 'grad_norm': 1.2713936397482426, 'learning_rate': 4.667587006670249e-07, 'epoch': 2.13}
{'loss': 0.7021, 'grad_norm': 1.3581696105691439, 'learning_rate': 4.568420154696164e-07, 'epoch': 2.14}
{'loss': 0.7219, 'grad_norm': 1.3741635929607523, 'learning_rate': 4.470005269026482e-07, 'epoch': 2.15}
{'loss': 0.7195, 'grad_norm': 1.3521156700297485, 'learning_rate': 4.372355974551183e-07, 'epoch': 2.16}
{'loss': 0.7185, 'grad_norm': 1.434998772718351, 'learning_rate': 4.275485790169234e-07, 'epoch': 2.17}
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-10-30 03:40:30,873 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 03:40:30,873 >>   Batch size = 2
 74%|████████████████████████████████████████████████▏                | 2200/2967 [7:31:02<1:57:28,  9.19s/it][INFO|trainer.py:4226] 2025-10-30 03:50:36,202 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.76979660987854, 'eval_runtime': 123.9636, 'eval_samples_per_second': 13.439, 'eval_steps_per_second': 0.847, 'epoch': 2.17}
{'loss': 0.6756, 'grad_norm': 1.4040012908228925, 'learning_rate': 4.1794081269169455e-07, 'epoch': 2.18}
{'loss': 0.6879, 'grad_norm': 1.439251636620931, 'learning_rate': 4.084136286111323e-07, 'epoch': 2.19}
{'loss': 0.6793, 'grad_norm': 1.3531164775618398, 'learning_rate': 3.9896834575085904e-07, 'epoch': 2.2}
{'loss': 0.7243, 'grad_norm': 1.2925904515811357, 'learning_rate': 3.8960627174781424e-07, 'epoch': 2.21}
{'loss': 0.6893, 'grad_norm': 1.3294675885175908, 'learning_rate': 3.8032870271922126e-07, 'epoch': 2.22}
[INFO|trainer.py:4228] 2025-10-30 03:50:36,202 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 03:50:36,202 >>   Batch size = 2
 76%|█████████████████████████████████████████████████▎               | 2250/2967 [7:41:06<1:55:53,  9.70s/it][INFO|trainer.py:4226] 2025-10-30 04:00:39,655 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7681160569190979, 'eval_runtime': 123.9517, 'eval_samples_per_second': 13.441, 'eval_steps_per_second': 0.847, 'epoch': 2.22}
{'loss': 0.7009, 'grad_norm': 1.4288766484402053, 'learning_rate': 3.711369230831496e-07, 'epoch': 2.23}
{'loss': 0.6936, 'grad_norm': 1.3959457868258838, 'learning_rate': 3.620322053806941e-07, 'epoch': 2.24}
{'loss': 0.6909, 'grad_norm': 1.2979857357721198, 'learning_rate': 3.530158100998003e-07, 'epoch': 2.25}
{'loss': 0.6844, 'grad_norm': 1.3922891188953215, 'learning_rate': 3.4408898550076014e-07, 'epoch': 2.26}
{'loss': 0.7184, 'grad_norm': 1.4591268724674846, 'learning_rate': 3.352529674433957e-07, 'epoch': 2.28}
[INFO|trainer.py:4228] 2025-10-30 04:00:39,655 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 04:00:39,655 >>   Batch size = 2
 78%|██████████████████████████████████████████████████▍              | 2300/2967 [7:51:08<1:40:38,  9.05s/it][INFO|trainer.py:4226] 2025-10-30 04:10:41,866 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7696107625961304, 'eval_runtime': 123.8704, 'eval_samples_per_second': 13.45, 'eval_steps_per_second': 0.848, 'epoch': 2.28}
{'loss': 0.6945, 'grad_norm': 1.385589750157947, 'learning_rate': 3.265089792159644e-07, 'epoch': 2.29}
{'loss': 0.7043, 'grad_norm': 1.3416236131788537, 'learning_rate': 3.1785823136580246e-07, 'epoch': 2.3}
{'loss': 0.6887, 'grad_norm': 1.34276558243707, 'learning_rate': 3.0930192153173227e-07, 'epoch': 2.31}
{'loss': 0.7174, 'grad_norm': 1.380060569436239, 'learning_rate': 3.008412342782585e-07, 'epoch': 2.32}
{'loss': 0.6726, 'grad_norm': 1.3887914429751553, 'learning_rate': 2.924773409315707e-07, 'epoch': 2.33}
[INFO|trainer.py:4228] 2025-10-30 04:10:41,866 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 04:10:41,866 >>   Batch size = 2
 79%|███████████████████████████████████████████████████▍             | 2350/2967 [8:01:14<1:37:41,  9.50s/it][INFO|trainer.py:4226] 2025-10-30 04:20:47,861 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7702994346618652, 'eval_runtime': 124.0085, 'eval_samples_per_second': 13.435, 'eval_steps_per_second': 0.847, 'epoch': 2.33}
{'loss': 0.7055, 'grad_norm': 1.349860487674876, 'learning_rate': 2.842113994173829e-07, 'epoch': 2.34}
{'loss': 0.698, 'grad_norm': 1.377990543675596, 'learning_rate': 2.760445541006261e-07, 'epoch': 2.35}
{'loss': 0.6742, 'grad_norm': 1.5613431823524562, 'learning_rate': 2.6797793562701754e-07, 'epoch': 2.36}
{'loss': 0.6986, 'grad_norm': 1.3667490988276054, 'learning_rate': 2.6001266076653096e-07, 'epoch': 2.37}
{'loss': 0.7173, 'grad_norm': 1.4334605580708382, 'learning_rate': 2.5214983225878796e-07, 'epoch': 2.38}
[INFO|trainer.py:4228] 2025-10-30 04:20:47,861 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 04:20:47,862 >>   Batch size = 2
 81%|████████████████████████████████████████████████████▌            | 2400/2967 [8:11:18<1:24:43,  8.97s/it][INFO|trainer.py:4226] 2025-10-30 04:30:52,276 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.767677366733551, 'eval_runtime': 123.9014, 'eval_samples_per_second': 13.446, 'eval_steps_per_second': 0.847, 'epoch': 2.38}
{'loss': 0.7173, 'grad_norm': 1.4028211860734356, 'learning_rate': 2.4439053866038937e-07, 'epoch': 2.39}
{'loss': 0.706, 'grad_norm': 1.384726668891871, 'learning_rate': 2.3673585419421273e-07, 'epoch': 2.4}
{'loss': 0.6863, 'grad_norm': 1.3729606319879184, 'learning_rate': 2.2918683860069387e-07, 'epoch': 2.41}
{'loss': 0.7254, 'grad_norm': 1.4801987391715306, 'learning_rate': 2.2174453699111096e-07, 'epoch': 2.42}
{'loss': 0.7046, 'grad_norm': 1.4933286298755508, 'learning_rate': 2.1440997970289743e-07, 'epoch': 2.43}
[INFO|trainer.py:4228] 2025-10-30 04:30:52,276 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 04:30:52,276 >>   Batch size = 2
 81%|████████████████████████████████████████████████████▌            | 2400/2967 [8:13:22<1:24:43,  8.97s/it][INFO|trainer.py:3910] 2025-10-30 04:33:01,222 >> Saving model checkpoint to /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2400
[INFO|configuration_utils.py:420] 2025-10-30 04:33:01,248 >> Configuration saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2400/config.json
{'eval_loss': 0.7678543925285339, 'eval_runtime': 123.9529, 'eval_samples_per_second': 13.441, 'eval_steps_per_second': 0.847, 'epoch': 2.43}
[INFO|configuration_utils.py:909] 2025-10-30 04:33:01,261 >> Configuration saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2400/generation_config.json
[INFO|modeling_utils.py:2996] 2025-10-30 04:33:16,864 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2400/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2491] 2025-10-30 04:33:16,875 >> tokenizer config file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-10-30 04:33:16,883 >> Special tokens file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2400/special_tokens_map.json
[2025-10-30 04:33:17,559] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step2400 is about to be saved!
[2025-10-30 04:33:17,574] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2400/global_step2400/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-10-30 04:33:17,574] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2400/global_step2400/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-10-30 04:33:17,624] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2400/global_step2400/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-10-30 04:33:17,657] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2400/global_step2400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-10-30 04:34:05,843] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2400/global_step2400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-10-30 04:34:05,853] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2400/global_step2400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-10-30 04:34:05,948] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2400 is ready now!
 83%|█████████████████████████████████████████████████████▋           | 2450/2967 [8:22:34<1:22:59,  9.63s/it][INFO|trainer.py:4226] 2025-10-30 04:42:07,792 >>
{'loss': 0.7001, 'grad_norm': 1.2178082028807151, 'learning_rate': 2.0718418215699862e-07, 'epoch': 2.44}
{'loss': 0.7099, 'grad_norm': 1.5637328439328135, 'learning_rate': 2.0006814471729205e-07, 'epoch': 2.45}
{'loss': 0.7163, 'grad_norm': 1.428661289129498, 'learning_rate': 1.9306285255209465e-07, 'epoch': 2.46}
{'loss': 0.7226, 'grad_norm': 1.4484441463268798, 'learning_rate': 1.8616927549777328e-07, 'epoch': 2.47}
{'loss': 0.7047, 'grad_norm': 1.4526830694529196, 'learning_rate': 1.7938836792447577e-07, 'epoch': 2.48}
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-10-30 04:42:07,792 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 04:42:07,792 >>   Batch size = 2
 84%|██████████████████████████████████████████████████████▊          | 2500/2967 [8:32:42<1:14:30,  9.57s/it][INFO|trainer.py:4226] 2025-10-30 04:52:16,214 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7674477100372314, 'eval_runtime': 123.9775, 'eval_samples_per_second': 13.438, 'eval_steps_per_second': 0.847, 'epoch': 2.48}
{'loss': 0.7224, 'grad_norm': 1.3004615329502154, 'learning_rate': 1.727210686040057e-07, 'epoch': 2.49}
{'loss': 0.7008, 'grad_norm': 1.4137719871421757, 'learning_rate': 1.6616830057985598e-07, 'epoch': 2.5}
{'loss': 0.6961, 'grad_norm': 1.3526739279244395, 'learning_rate': 1.5973097103941847e-07, 'epoch': 2.51}
{'loss': 0.7084, 'grad_norm': 1.4715919260963977, 'learning_rate': 1.534099711883905e-07, 'epoch': 2.52}
{'loss': 0.7047, 'grad_norm': 1.40504602255021, 'learning_rate': 1.4720617612739416e-07, 'epoch': 2.53}
[INFO|trainer.py:4228] 2025-10-30 04:52:16,215 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 04:52:16,215 >>   Batch size = 2
 86%|███████████████████████████████████████████████████████▊         | 2550/2967 [8:42:46<1:06:45,  9.60s/it][INFO|trainer.py:4226] 2025-10-30 05:02:20,233 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7673734426498413, 'eval_runtime': 123.9608, 'eval_samples_per_second': 13.44, 'eval_steps_per_second': 0.847, 'epoch': 2.53}
{'loss': 0.7098, 'grad_norm': 1.3716342662979442, 'learning_rate': 1.4112044473082207e-07, 'epoch': 2.54}
{'loss': 0.6912, 'grad_norm': 1.373718612812681, 'learning_rate': 1.3515361952793492e-07, 'epoch': 2.55}
{'loss': 0.7316, 'grad_norm': 1.3936749625179794, 'learning_rate': 1.2930652658621642e-07, 'epoch': 2.56}
{'loss': 0.6722, 'grad_norm': 1.3623737864337706, 'learning_rate': 1.2357997539701082e-07, 'epoch': 2.57}
{'loss': 0.686, 'grad_norm': 1.3838764388009284, 'learning_rate': 1.179747587634543e-07, 'epoch': 2.58}
[INFO|trainer.py:4228] 2025-10-30 05:02:20,233 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 05:02:20,234 >>   Batch size = 2
 88%|████████████████████████████████████████████████████████▉        | 2600/2967 [8:52:56<1:00:55,  9.96s/it][INFO|trainer.py:4226] 2025-10-30 05:12:29,760 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7671611905097961, 'eval_runtime': 123.9264, 'eval_samples_per_second': 13.443, 'eval_steps_per_second': 0.847, 'epoch': 2.58}
{'loss': 0.6966, 'grad_norm': 1.3800413666093703, 'learning_rate': 1.1249165269071615e-07, 'epoch': 2.59}
{'loss': 0.6877, 'grad_norm': 1.4599754911136475, 'learning_rate': 1.0713141627856581e-07, 'epoch': 2.6}
{'loss': 0.6799, 'grad_norm': 1.337397049366967, 'learning_rate': 1.0189479161628167e-07, 'epoch': 2.61}
{'loss': 0.7041, 'grad_norm': 1.454460140315698, 'learning_rate': 9.678250367991236e-08, 'epoch': 2.62}
{'loss': 0.7024, 'grad_norm': 1.4332618968869701, 'learning_rate': 9.179526023190908e-08, 'epoch': 2.63}
[INFO|trainer.py:4228] 2025-10-30 05:12:29,760 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 05:12:29,760 >>   Batch size = 2
 89%|███████████████████████████████████████████████████████████▊       | 2650/2967 [9:02:58<49:34,  9.38s/it][INFO|trainer.py:4226] 2025-10-30 05:22:32,237 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7676341533660889, 'eval_runtime': 124.0514, 'eval_samples_per_second': 13.43, 'eval_steps_per_second': 0.846, 'epoch': 2.63}
{'loss': 0.6808, 'grad_norm': 1.4880905018302863, 'learning_rate': 8.693375172314133e-08, 'epoch': 2.64}
{'loss': 0.7122, 'grad_norm': 1.4805099794873278, 'learning_rate': 8.219865119730728e-08, 'epoch': 2.65}
{'loss': 0.7395, 'grad_norm': 1.4286574090953266, 'learning_rate': 7.759061419775615e-08, 'epoch': 2.66}
{'loss': 0.7091, 'grad_norm': 1.4504426877438714, 'learning_rate': 7.311027867673237e-08, 'epoch': 2.67}
{'loss': 0.7327, 'grad_norm': 1.2740653514378375, 'learning_rate': 6.875826490705516e-08, 'epoch': 2.68}
[INFO|trainer.py:4228] 2025-10-30 05:22:32,237 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 05:22:32,238 >>   Batch size = 2
 91%|████████████████████████████████████████████████████████████▉      | 2700/2967 [9:13:04<41:44,  9.38s/it][INFO|trainer.py:4226] 2025-10-30 05:32:37,906 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7668251395225525, 'eval_runtime': 123.8865, 'eval_samples_per_second': 13.448, 'eval_steps_per_second': 0.848, 'epoch': 2.68}
{'loss': 0.7078, 'grad_norm': 1.4417835739990177, 'learning_rate': 6.453517539624642e-08, 'epoch': 2.69}
{'loss': 0.704, 'grad_norm': 1.3686619497940444, 'learning_rate': 6.044159480311673e-08, 'epoch': 2.7}
{'loss': 0.7066, 'grad_norm': 1.4918698940478614, 'learning_rate': 5.647808985682367e-08, 'epoch': 2.71}
{'loss': 0.6961, 'grad_norm': 1.5069562048972636, 'learning_rate': 5.264520927841187e-08, 'epoch': 2.72}
{'loss': 0.6801, 'grad_norm': 1.5421272332836993, 'learning_rate': 4.8943483704846465e-08, 'epoch': 2.73}
[INFO|trainer.py:4228] 2025-10-30 05:32:37,906 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 05:32:37,906 >>   Batch size = 2
 91%|████████████████████████████████████████████████████████████▉      | 2700/2967 [9:15:08<41:44,  9.38s/it][INFO|trainer.py:3910] 2025-10-30 05:34:46,959 >> Saving model checkpoint to /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2700
[INFO|configuration_utils.py:420] 2025-10-30 05:34:46,984 >> Configuration saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2700/config.json
{'eval_loss': 0.766968846321106, 'eval_runtime': 123.9734, 'eval_samples_per_second': 13.438, 'eval_steps_per_second': 0.847, 'epoch': 2.73}
[INFO|configuration_utils.py:909] 2025-10-30 05:34:46,994 >> Configuration saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2700/generation_config.json
[INFO|modeling_utils.py:2996] 2025-10-30 05:35:02,597 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2700/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2491] 2025-10-30 05:35:02,609 >> tokenizer config file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-10-30 05:35:02,617 >> Special tokens file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2700/special_tokens_map.json
[2025-10-30 05:35:03,302] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step2700 is about to be saved!
[2025-10-30 05:35:03,317] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2700/global_step2700/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-10-30 05:35:03,318] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2700/global_step2700/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-10-30 05:35:03,376] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2700/global_step2700/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-10-30 05:35:03,402] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2700/global_step2700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-10-30 05:35:50,075] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2700/global_step2700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-10-30 05:35:50,087] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2700/global_step2700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-10-30 05:35:50,583] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2700 is ready now!
 93%|██████████████████████████████████████████████████████████████     | 2750/2967 [9:24:19<36:49, 10.18s/it][INFO|trainer.py:4226] 2025-10-30 05:43:52,958 >>
{'loss': 0.7338, 'grad_norm': 1.3761676575152662, 'learning_rate': 4.5373425615549244e-08, 'epoch': 2.74}
{'loss': 0.6915, 'grad_norm': 1.1435539909135362, 'learning_rate': 4.193552926144972e-08, 'epoch': 2.75}
{'loss': 0.7012, 'grad_norm': 1.4602282188174225, 'learning_rate': 3.863027059655977e-08, 'epoch': 2.76}
{'loss': 0.6989, 'grad_norm': 1.5474796457671425, 'learning_rate': 3.545810721207987e-08, 'epoch': 2.77}
{'loss': 0.6985, 'grad_norm': 1.5248488158059168, 'learning_rate': 3.241947827304947e-08, 'epoch': 2.78}
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-10-30 05:43:52,958 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 05:43:52,959 >>   Batch size = 2
 94%|███████████████████████████████████████████████████████████████▏   | 2800/2967 [9:34:23<26:30,  9.53s/it][INFO|trainer.py:4226] 2025-10-30 05:53:56,685 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7673956155776978, 'eval_runtime': 123.9831, 'eval_samples_per_second': 13.437, 'eval_steps_per_second': 0.847, 'epoch': 2.78}
{'loss': 0.6942, 'grad_norm': 1.422204202646509, 'learning_rate': 2.9514804457547505e-08, 'epoch': 2.79}
{'loss': 0.7202, 'grad_norm': 1.443191077848153, 'learning_rate': 2.6744487898451517e-08, 'epoch': 2.8}
{'loss': 0.6745, 'grad_norm': 1.381525933178815, 'learning_rate': 2.4108912127766112e-08, 'epoch': 2.81}
{'loss': 0.6762, 'grad_norm': 1.4773087199946406, 'learning_rate': 2.160844202352474e-08, 'epoch': 2.82}
{'loss': 0.6908, 'grad_norm': 1.4857509309437034, 'learning_rate': 1.924342375927468e-08, 'epoch': 2.83}
[INFO|trainer.py:4228] 2025-10-30 05:53:56,685 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 05:53:56,685 >>   Batch size = 2
 96%|████████████████████████████████████████████████████████████████▎  | 2850/2967 [9:44:26<19:21,  9.93s/it][INFO|trainer.py:4226] 2025-10-30 06:04:00,418 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7668167352676392, 'eval_runtime': 124.0309, 'eval_samples_per_second': 13.432, 'eval_steps_per_second': 0.847, 'epoch': 2.83}
{'loss': 0.6964, 'grad_norm': 1.5179707170651, 'learning_rate': 1.701418475615224e-08, 'epoch': 2.84}
{'loss': 0.7114, 'grad_norm': 1.400758796611656, 'learning_rate': 1.4921033637552927e-08, 'epoch': 2.85}
{'loss': 0.6708, 'grad_norm': 1.4903817661398775, 'learning_rate': 1.2964260186404618e-08, 'epoch': 2.86}
{'loss': 0.6788, 'grad_norm': 1.4645840051706227, 'learning_rate': 1.1144135305049429e-08, 'epoch': 2.87}
{'loss': 0.7185, 'grad_norm': 1.3852315897954781, 'learning_rate': 9.46091097773849e-09, 'epoch': 2.88}
[INFO|trainer.py:4228] 2025-10-30 06:04:00,419 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 06:04:00,419 >>   Batch size = 2
 98%|█████████████████████████████████████████████████████████████████▍ | 2900/2967 [9:54:30<10:46,  9.65s/it][INFO|trainer.py:4226] 2025-10-30 06:14:03,787 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7668318152427673, 'eval_runtime': 123.9589, 'eval_samples_per_second': 13.44, 'eval_steps_per_second': 0.847, 'epoch': 2.88}
{'loss': 0.7002, 'grad_norm': 1.4087348764672072, 'learning_rate': 7.914820235746856e-09, 'epoch': 2.89}
{'loss': 0.6915, 'grad_norm': 1.4223038598632385, 'learning_rate': 6.506077125111864e-09, 'epoch': 2.9}
{'loss': 0.7008, 'grad_norm': 1.3024846445370764, 'learning_rate': 5.234876676999289e-09, 'epoch': 2.91}
{'loss': 0.7312, 'grad_norm': 1.338257164637543, 'learning_rate': 4.101394880703602e-09, 'epoch': 2.92}
{'loss': 0.7306, 'grad_norm': 1.4502037891010329, 'learning_rate': 3.105788659282349e-09, 'epoch': 2.93}
[INFO|trainer.py:4228] 2025-10-30 06:14:03,787 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 06:14:03,787 >>   Batch size = 2
 99%|█████████████████████████████████████████████████████████████████▌| 2950/2967 [10:04:35<02:39,  9.41s/it][INFO|trainer.py:4226] 2025-10-30 06:24:08,866 >>
***** Running Evaluation *****                                                                                
{'eval_loss': 0.7667268514633179, 'eval_runtime': 124.0397, 'eval_samples_per_second': 13.431, 'eval_steps_per_second': 0.847, 'epoch': 2.93}
{'loss': 0.6777, 'grad_norm': 1.286511063236251, 'learning_rate': 2.2481958478317574e-09, 'epoch': 2.94}
{'loss': 0.6793, 'grad_norm': 1.5414625407245395, 'learning_rate': 1.5287351744044386e-09, 'epoch': 2.95}
{'loss': 0.7071, 'grad_norm': 1.41320054060454, 'learning_rate': 9.475062435719828e-10, 'epoch': 2.96}
{'loss': 0.695, 'grad_norm': 1.3864791691084357, 'learning_rate': 5.045895226355456e-10, 'epoch': 2.97}
{'loss': 0.6927, 'grad_norm': 1.3723144071757587, 'learning_rate': 2.0004633048564811e-10, 'epoch': 2.98}
[INFO|trainer.py:4228] 2025-10-30 06:24:08,866 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 06:24:08,866 >>   Batch size = 2
100%|██████████████████████████████████████████████████████████████████| 2967/2967 [10:09:19<00:00,  9.71s/it][INFO|trainer.py:3910] 2025-10-30 06:28:58,985 >> Saving model checkpoint to /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2967
[INFO|configuration_utils.py:420] 2025-10-30 06:28:59,011 >> Configuration saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2967/config.json
{'eval_loss': 0.7666947245597839, 'eval_runtime': 123.9624, 'eval_samples_per_second': 13.44, 'eval_steps_per_second': 0.847, 'epoch': 2.98}
{'loss': 0.6587, 'grad_norm': 1.4283286945153582, 'learning_rate': 3.3918829112966615e-11, 'epoch': 2.99}
[INFO|configuration_utils.py:909] 2025-10-30 06:28:59,024 >> Configuration saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2967/generation_config.json
[INFO|modeling_utils.py:2996] 2025-10-30 06:29:14,430 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2967/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2491] 2025-10-30 06:29:14,442 >> tokenizer config file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2967/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-10-30 06:29:14,453 >> Special tokens file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2967/special_tokens_map.json
[2025-10-30 06:29:15,189] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step2967 is about to be saved!
[2025-10-30 06:29:15,204] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2967/global_step2967/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-10-30 06:29:15,204] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2967/global_step2967/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-10-30 06:29:15,262] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2967/global_step2967/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-10-30 06:29:15,287] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2967/global_step2967/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-10-30 06:30:01,860] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2967/global_step2967/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-10-30 06:30:01,872] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/checkpoint-2967/global_step2967/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-10-30 06:30:02,661] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2967 is ready now!
[INFO|trainer.py:2643] 2025-10-30 06:30:02,763 >>

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|██████████████████████████████████████████████████████████████████| 2967/2967 [10:10:29<00:00, 12.35s/it]
{'train_runtime': 36631.3845, 'train_samples_per_second': 2.591, 'train_steps_per_second': 0.081, 'train_loss': 0.7792410431062489, 'epoch': 3.0}
[INFO|trainer.py:3910] 2025-10-30 06:30:08,052 >> Saving model checkpoint to /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027
[INFO|configuration_utils.py:420] 2025-10-30 06:30:08,064 >> Configuration saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/config.json
[INFO|configuration_utils.py:909] 2025-10-30 06:30:08,094 >> Configuration saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/generation_config.json
[INFO|modeling_utils.py:2996] 2025-10-30 06:30:23,588 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2491] 2025-10-30 06:30:23,621 >> tokenizer config file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-10-30 06:30:23,649 >> Special tokens file saved in /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/special_tokens_map.json
***** train metrics *****
  epoch                    =         3.0
  total_flos               =    384624GF
  train_loss               =      0.7792
  train_runtime            = 10:10:31.38
  train_samples_per_second =       2.591
  train_steps_per_second   =       0.081
Figure saved at: /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/training_loss.png
Figure saved at: /apdcephfs_cq11/share_1567347/share_info/rhyang/AdaAgent/models/llama3.1-8b_cog-sft_verl_v2_alf_lr2e6_bs32_epoch3_full_1027/training_eval_loss.png
[WARNING|2025-10-30 06:30:24] llamafactory.extras.ploting:162 >> No metric eval_accuracy to plot.
[INFO|trainer.py:4226] 2025-10-30 06:30:24,650 >>
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-10-30 06:30:24,651 >>   Num examples = 1666
[INFO|trainer.py:4231] 2025-10-30 06:30:24,651 >>   Batch size = 2
100%|███████████████████████████████████████████████████████████████████████| 105/105 [02:03<00:00,  1.17s/it]
***** eval metrics *****
  epoch                   =        3.0
  eval_loss               =     0.7667
  eval_runtime            = 0:02:04.29
  eval_samples_per_second =     13.403
  eval_steps_per_second   =      0.845
[INFO|modelcard.py:449] 2025-10-30 06:32:29,016 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
