  0%|                                                                                | 0/1355 [00:00<?, ?it/s]/root/miniconda3/envs/lf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  4%|██▌                                                                  | 50/1355 [02:44<1:10:02,  3.22s/it][INFO|trainer.py:3819] 2025-07-10 13:45:53,654 >>
[2025-07-10 13:43:25,545] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5799, 'grad_norm': 6.413904664612279, 'learning_rate': 1.4705882352941175e-07, 'epoch': 0.04}
{'loss': 0.5283, 'grad_norm': 5.258558595235458, 'learning_rate': 2.941176470588235e-07, 'epoch': 0.07}
{'loss': 0.5262, 'grad_norm': 3.745726512062614, 'learning_rate': 4.4117647058823526e-07, 'epoch': 0.11}
{'loss': 0.4464, 'grad_norm': 2.086717523228333, 'learning_rate': 5.88235294117647e-07, 'epoch': 0.15}
{'loss': 0.438, 'grad_norm': 2.2355933944282076, 'learning_rate': 7.352941176470589e-07, 'epoch': 0.18}
***** Running Evaluation *****
[INFO|trainer.py:3821] 2025-07-10 13:45:53,654 >>   Num examples = 114
[INFO|trainer.py:3824] 2025-07-10 13:45:53,654 >>   Batch size = 2
Traceback (most recent call last):
  File "/apdcephfs_cq11/share_1567347/share_info/rhyang/LLaMA-Factory/src/llamafactory/launcher.py", line 23, in <module>
    launch()
  File "/apdcephfs_cq11/share_1567347/share_info/rhyang/LLaMA-Factory/src/llamafactory/launcher.py", line 19, in launch
    run_exp()
  File "/apdcephfs_cq11/share_1567347/share_info/rhyang/LLaMA-Factory/src/llamafactory/train/tuner.py", line 50, in run_exp
    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
  File "/apdcephfs_cq11/share_1567347/share_info/rhyang/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 96, in run_sft
    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/transformers/trainer.py", line 1938, in train
    return inner_training_loop(
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/transformers/trainer.py", line 2356, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/transformers/trainer.py", line 2804, in _maybe_log_save_evaluate
    metrics = self._evaluate(trial, ignore_keys_for_eval)
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/transformers/trainer.py", line 2761, in _evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/transformers/trainer_seq2seq.py", line 180, in evaluate
    return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/transformers/trainer.py", line 3666, in evaluate
    output = eval_loop(
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/transformers/trainer.py", line 3857, in evaluation_loop
    losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
  File "/apdcephfs_cq11/share_1567347/share_info/rhyang/LLaMA-Factory/src/llamafactory/train/sft/trainer.py", line 121, in prediction_step
    loss, generated_tokens, _ = super().prediction_step(  # ignore the returned labels (may be truncated)
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/transformers/trainer_seq2seq.py", line 278, in prediction_step
    return super().prediction_step(
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/transformers/trainer.py", line 4075, in prediction_step
    loss, outputs = self.compute_loss(model, inputs, return_outputs=True)
  File "/apdcephfs_cq11/share_1567347/share_info/rhyang/LLaMA-Factory/src/llamafactory/train/sft/trainer.py", line 88, in compute_loss
    loss = super().compute_loss(model, inputs, return_outputs, **kwargs)
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/transformers/trainer.py", line 3363, in compute_loss
    outputs = model(**inputs)
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1561, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 1082, in forward
    loss = loss_fct(shift_logits, shift_labels)
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/torch/nn/modules/loss.py", line 1179, in forward
    return F.cross_entropy(input, target, weight=self.weight,
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/torch/nn/functional.py", line 3059, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.69 GiB. GPU 0 has a total capacity of 39.59 GiB of which 6.10 GiB is free. Process 51617 has 33.49 GiB memory in use. Of the allocated memory 28.12 GiB is allocated by PyTorch, and 2.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/apdcephfs_cq11/share_1567347/share_info/rhyang/LLaMA-Factory/src/llamafactory/launcher.py", line 23, in <module>
    launch()
  File "/apdcephfs_cq11/share_1567347/share_info/rhyang/LLaMA-Factory/src/llamafactory/launcher.py", line 19, in launch
    run_exp()
  File "/apdcephfs_cq11/share_1567347/share_info/rhyang/LLaMA-Factory/src/llamafactory/train/tuner.py", line 50, in run_exp
    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
  File "/apdcephfs_cq11/share_1567347/share_info/rhyang/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 96, in run_sft
    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/transformers/trainer.py", line 1938, in train
    return inner_training_loop(
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/transformers/trainer.py", line 2356, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/transformers/trainer.py", line 2804, in _maybe_log_save_evaluate
    metrics = self._evaluate(trial, ignore_keys_for_eval)
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/transformers/trainer.py", line 2761, in _evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/transformers/trainer_seq2seq.py", line 180, in evaluate
    return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/transformers/trainer.py", line 3666, in evaluate
    output = eval_loop(
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/transformers/trainer.py", line 3857, in evaluation_loop
    losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
  File "/apdcephfs_cq11/share_1567347/share_info/rhyang/LLaMA-Factory/src/llamafactory/train/sft/trainer.py", line 121, in prediction_step
    loss, generated_tokens, _ = super().prediction_step(  # ignore the returned labels (may be truncated)
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/transformers/trainer_seq2seq.py", line 278, in prediction_step
    return super().prediction_step(
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/transformers/trainer.py", line 4075, in prediction_step
    loss, outputs = self.compute_loss(model, inputs, return_outputs=True)
  File "/apdcephfs_cq11/share_1567347/share_info/rhyang/LLaMA-Factory/src/llamafactory/train/sft/trainer.py", line 88, in compute_loss
    loss = super().compute_loss(model, inputs, return_outputs, **kwargs)
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/transformers/trainer.py", line 3363, in compute_loss
    outputs = model(**inputs)
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1561, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 1082, in forward
    loss = loss_fct(shift_logits, shift_labels)
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/torch/nn/modules/loss.py", line 1179, in forward
    return F.cross_entropy(input, target, weight=self.weight,
  File "/root/miniconda3/envs/lf/lib/python3.10/site-packages/torch/nn/functional.py", line 3059, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.69 GiB. GPU 0 has a total capacity of 39.59 GiB of which 6.10 GiB is free. Process 51617 has 33.49 GiB memory in use. Of the allocated memory 28.12 GiB is allocated by PyTorch, and 2.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
